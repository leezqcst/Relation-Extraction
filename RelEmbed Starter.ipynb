{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import collections\n",
    "import random\n",
    "from time import time\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA, FastICA\n",
    "\n",
    "import data_handler as dh\n",
    "import semeval_data_helper as sdh\n",
    "\n",
    "\n",
    "# plot settings\n",
    "% matplotlib inline\n",
    "# print(plt.rcParams.keys())\n",
    "# plt.rcParams['figure.figsize'] = (16,9)\n",
    "\n",
    "import mpld3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# reload(sdh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reload(nn)\n",
    "import relembed as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# reload(eh)\n",
    "import experiment_helper as eh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shuffle_seed = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Data objects...\n",
      "Done creating Data objects\n",
      "7999 total examples :: 7199 training : 800 valid (90:10 split)\n",
      "Vocab size: 22683 Dep size: 50\n"
     ]
    }
   ],
   "source": [
    "reload(dh)\n",
    "DH = dh.DataHandler('data/semeval_train_sdp_8000', valid_percent=10, shuffle_seed=shuffle_seed) # for semeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Didn't find common ancestor\n",
      "1790\t\"The imams were removed from a US Airways <e1>flight</e1> awaiting departure from the Minneapolis-St. Paul <e2>airport</e2>.\"\n",
      "\n",
      "(The imams were removed from a US Airways flight awaiting departure from the Minneapolis - St . Paul airport ., flight , airport )\n",
      "Bad sentence: '1790\\t\"The imams were removed from a US Airways <e1>flight</e1> awaiting departure from the Minneapolis-St. Paul <e2>airport</e2>.\"\\r\\n'\n",
      "((The imams were removed from a US Airways flight awaiting departure from the Minneapolis - St . Paul airport ., flight , airport ), None)\n",
      "Skipping this one... '1790\\t\"The imams were removed from a US Airways <e1>flight</e1> awaiting departure from the Minneapolis-St. Paul <e2>airport</e2>.\"\\r\\n'\n",
      "(None, None, None, 4)\n",
      "Num training: 7199\n",
      "Num valididation: 800\n",
      "Didn't find common ancestor\n",
      "8310\t\"Tributes have been paid to the <e1>writer</e1> who created Goodness Gracious Me, the hit BBC television <e2>series</e2>.\"\n",
      "\n",
      "(Tributes have been paid to the writer who created Goodness Gracious Me , the hit BBC television series ., writer , series )\n",
      "Bad sentence: '8310\\t\"Tributes have been paid to the <e1>writer</e1> who created Goodness Gracious Me, the hit BBC television <e2>series</e2>.\"\\r\\n'\n",
      "((Tributes have been paid to the writer who created Goodness Gracious Me , the hit BBC television series ., writer , series ), None)\n",
      "Skipping this one... '8000\\t\"The <e1>surgeon</e1> cuts a small <e2>hole</e2> in the skull and lifts the edge of the brain to expose the nerve.\"\\r\\n'\n",
      "(None, None, None, 3)\n",
      "Num testing: 2717\n"
     ]
    }
   ],
   "source": [
    "# reload(sdh)\n",
    "train, valid, test, label2int, int2label = sdh.load_semeval_data(include_ends=False, shuffle_seed=shuffle_seed)\n",
    "num_classes = len(int2label.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[180, 134], [150, 18979], [2494, 305], [769, 911], [2319, 1166]]\n"
     ]
    }
   ],
   "source": [
    "# convert the semeval data to indices under the wiki vocab:\n",
    "train['sdps'] = DH.sentences_to_sequences(train['sdps'])\n",
    "valid['sdps'] = DH.sentences_to_sequences(valid['sdps'])\n",
    "test['sdps'] = DH.sentences_to_sequences(test['sdps'])\n",
    "    \n",
    "train['targets'] = DH.sentences_to_sequences(train['targets'])\n",
    "valid['targets'] = DH.sentences_to_sequences(valid['targets'])\n",
    "test['targets'] = DH.sentences_to_sequences(test['targets'])\n",
    "\n",
    "print(train['targets'][:5]) # small sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 13\n"
     ]
    }
   ],
   "source": [
    "max_seq_len = max([len(path) for path in train['sdps']+valid['sdps']+test['sdps']])\n",
    "print(max_seq_len, DH.max_seq_len)\n",
    "DH.max_seq_len = max_seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19423 / 22683 pretrained\n"
     ]
    }
   ],
   "source": [
    "# the embedding matrix is started of as random uniform [-1,1]\n",
    "# then we replace everything but the OOV tokens with the approprate google vector\n",
    "fname = 'data/GoogleNews-vectors-negative300.bin'\n",
    "word2vec = Word2Vec.load_word2vec_format(fname, binary=True)\n",
    "\n",
    "word_embeddings = np.random.uniform(low=-1., high=1., size=[DH.vocab_size, 300]).astype(np.float32)\n",
    "num_found = 0\n",
    "for i, token in enumerate(DH.vocab):\n",
    "    if token in word2vec:\n",
    "        word_embeddings[i] = word2vec[token]\n",
    "        num_found += 1\n",
    "print(\"%i / %i pretrained\" % (num_found, DH.vocab_size))\n",
    "del word2vec # save a lot of RAM\n",
    "# normalize them\n",
    "# word_embeddings /= np.sqrt(np.sum(word_embeddings**2, 1, keepdims=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def reset_drnn(model_name='relembed', bi=True, dep_embed_size=25, word_embed_size=None, max_grad_norm=3.):\n",
    "    if word_embed_size:    \n",
    "        config = {\n",
    "            'max_num_steps':DH.max_seq_len,\n",
    "            'word_embed_size':word_embed_size,\n",
    "            'dep_embed_size':dep_embed_size,\n",
    "            'bidirectional':bi,\n",
    "            'hidden_layer_size':1000,\n",
    "            'vocab_size':DH.vocab_size,\n",
    "            'dep_vocab_size':DH.dep_size,\n",
    "            'num_predict_classes':num_classes,\n",
    "            'pretrained_word_embeddings':None,\n",
    "            'max_grad_norm':3.,\n",
    "            'model_name':model_name,\n",
    "            'checkpoint_prefix':'checkpoints/',\n",
    "            'summary_prefix':'tensor_summaries/'\n",
    "        }\n",
    "    else: # use pretrained google vectors\n",
    "        config = {\n",
    "            'max_num_steps':DH.max_seq_len,\n",
    "            'word_embed_size':300,\n",
    "            'dep_embed_size':dep_embed_size,\n",
    "            'bidirectional':bi,\n",
    "            'hidden_layer_size':1000,\n",
    "            'vocab_size':DH.vocab_size,\n",
    "            'dep_vocab_size':DH.dep_size,\n",
    "            'num_predict_classes':num_classes,\n",
    "            'pretrained_word_embeddings':word_embeddings,\n",
    "            'max_grad_norm':3.,\n",
    "            'model_name':model_name,\n",
    "            'checkpoint_prefix':'checkpoints/',\n",
    "            'summary_prefix':'tensor_summaries/'\n",
    "        }\n",
    "    try:\n",
    "        tf.reset_default_graph()\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        tf.get_default_session().close()\n",
    "    except:\n",
    "        pass\n",
    "    drnn = nn.RelEmbed(config)\n",
    "    print(drnn)\n",
    "    return drnn\n",
    "# drnn = reset_drnn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_validation_test(num_nearby=20):\n",
    "    valid_phrases, valid_targets , _, valid_lens = DH.validation_batch()\n",
    "    random_index = int(random.uniform(0, len(valid_lens)))\n",
    "    query_phrase = valid_phrases[random_index]\n",
    "    query_len = valid_lens[random_index]\n",
    "    query_target = valid_targets[random_index].reshape((1,2))\n",
    "    padded_qp = np.zeros([DH.max_seq_len, 2]).astype(np.int32)\n",
    "    padded_qp[:len(query_phrase), 0] = [x[0] for x in query_phrase]\n",
    "    padded_qp[:len(query_phrase), 1] = [x[1] for x in query_phrase]    \n",
    "    dists, phrase_idx = drnn.validation_phrase_nearby(padded_qp, query_len, query_target,\n",
    "                                                      valid_phrases, valid_lens, valid_targets)\n",
    "    print(\"=\"*80)\n",
    "    print(\"Top %i closest phrases to <%s> '%s' <%s>\" \n",
    "          % (num_nearby, DH.vocab_at(query_target[0,0]), \n",
    "             DH.sequence_to_sentence(query_phrase, query_len), \n",
    "             DH.vocab_at(query_target[0,1])))\n",
    "    for i in range(num_nearby):\n",
    "        dist = dists[i]\n",
    "        phrase = valid_phrases[phrase_idx[i]]\n",
    "        len_ = valid_lens[phrase_idx[i]]\n",
    "        target = valid_targets[phrase_idx[i]]\n",
    "        print(\"%i: %0.3f : <%s> '%s' <%s>\" \n",
    "              % (i, dist, DH.vocab_at(target[0]),\n",
    "                 DH.sequence_to_sentence(phrase, len_),\n",
    "                 DH.vocab_at(target[1])))\n",
    "    print(\"=\"*80)\n",
    "#     drnn.save_validation_accuracy(frac_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def time_left(num_epochs, num_steps, fit_time, nearby_time, start_time, nearby_mod):\n",
    "    total = num_epochs*num_steps*fit_time + ((num_epochs*num_steps)/float(nearby_mod))*nearby_time\n",
    "    return total - (time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
