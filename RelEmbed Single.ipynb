{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import collections\n",
    "import random\n",
    "from time import time\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA, FastICA\n",
    "\n",
    "import data_handler as dh\n",
    "import semeval_data_helper as sdh\n",
    "\n",
    "\n",
    "# plot settings\n",
    "% matplotlib inline\n",
    "# print(plt.rcParams.keys())\n",
    "# plt.rcParams['figure.figsize'] = (16,9)\n",
    "\n",
    "import mpld3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# reload(sdh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# reload(nn)\n",
    "import relembed_single as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# reload(eh)\n",
    "import experiment_helper as eh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shuffle_seed = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Data objects...\n",
      "Done creating Data objects\n",
      "471388 total examples :: 466674 training : 4714 valid (99:1 split)\n",
      "Vocab size: 27070 Dep size: 50 POS size: 18\n"
     ]
    }
   ],
   "source": [
    "reload(dh)\n",
    "DH = dh.DataHandler('data/semeval_wiki_sdp_include_single_50000', valid_percent=1, shuffle_seed=shuffle_seed) # for semeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Didn't find common ancestor\n",
      "1790\t\"The imams were removed from a US Airways <e1>flight</e1> awaiting departure from the Minneapolis-St. Paul <e2>airport</e2>.\"\n",
      "\n",
      "(The imams were removed from a US Airways flight awaiting departure from the Minneapolis - St . Paul airport ., flight , airport )\n",
      "Bad sentence: '1790\\t\"The imams were removed from a US Airways <e1>flight</e1> awaiting departure from the Minneapolis-St. Paul <e2>airport</e2>.\"\\r\\n'\n",
      "((The imams were removed from a US Airways flight awaiting departure from the Minneapolis - St . Paul airport ., flight , airport ), None)\n",
      "Skipping this one... '1790\\t\"The imams were removed from a US Airways <e1>flight</e1> awaiting departure from the Minneapolis-St. Paul <e2>airport</e2>.\"\\r\\n'\n",
      "(None, None, None, 4)\n",
      "Num training: 7199\n",
      "Num valididation: 800\n",
      "Didn't find common ancestor\n",
      "8310\t\"Tributes have been paid to the <e1>writer</e1> who created Goodness Gracious Me, the hit BBC television <e2>series</e2>.\"\n",
      "\n",
      "(Tributes have been paid to the writer who created Goodness Gracious Me , the hit BBC television series ., writer , series )\n",
      "Bad sentence: '8310\\t\"Tributes have been paid to the <e1>writer</e1> who created Goodness Gracious Me, the hit BBC television <e2>series</e2>.\"\\r\\n'\n",
      "((Tributes have been paid to the writer who created Goodness Gracious Me , the hit BBC television series ., writer , series ), None)\n",
      "Skipping this one... '8000\\t\"The <e1>surgeon</e1> cuts a small <e2>hole</e2> in the skull and lifts the edge of the brain to expose the nerve.\"\\r\\n'\n",
      "(None, None, None, 3)\n",
      "Num testing: 2717\n"
     ]
    }
   ],
   "source": [
    "reload(sdh)\n",
    "train, valid, test, label2int, int2label = sdh.load_semeval_data(include_ends=True, shuffle_seed=shuffle_seed, single=False)\n",
    "num_classes = len(int2label.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[237, 196], [351, 24436], [5596, 290], [1140, 630], [748, 2727]]\n"
     ]
    }
   ],
   "source": [
    "# convert the semeval data to indices under the wiki vocab:\n",
    "train['sdps'] = DH.sentences_to_sequences(train['sdps'])\n",
    "valid['sdps'] = DH.sentences_to_sequences(valid['sdps'])\n",
    "test['sdps'] = DH.sentences_to_sequences(test['sdps'])\n",
    "    \n",
    "train['targets'] = DH.sentences_to_sequences(train['targets'])\n",
    "valid['targets'] = DH.sentences_to_sequences(valid['targets'])\n",
    "test['targets'] = DH.sentences_to_sequences(test['targets'])\n",
    "\n",
    "print(train['targets'][:5]) # small sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 12\n"
     ]
    }
   ],
   "source": [
    "max_seq_len = max([len(path) for path in train['sdps']+valid['sdps']+test['sdps']])\n",
    "print(max_seq_len, DH.max_seq_len)\n",
    "DH.max_seq_len = max_seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22777 / 27070 pretrained\n"
     ]
    }
   ],
   "source": [
    "# the embedding matrix is started of as random uniform [-1,1]\n",
    "# then we replace everything but the OOV tokens with the approprate google vector\n",
    "fname = 'data/GoogleNews-vectors-negative300.bin'\n",
    "word2vec = Word2Vec.load_word2vec_format(fname, binary=True)\n",
    "\n",
    "word_embeddings = np.random.uniform(low=-1., high=1., size=[DH.vocab_size, 300]).astype(np.float32)\n",
    "num_found = 0\n",
    "for i, token in enumerate(DH.vocab):\n",
    "    if token in word2vec:\n",
    "        word_embeddings[i] = word2vec[token]\n",
    "        num_found += 1\n",
    "print(\"%i / %i pretrained\" % (num_found, DH.vocab_size))\n",
    "del word2vec # save a lot of RAM\n",
    "# normalize them\n",
    "word_embeddings /= np.sqrt(np.sum(word_embeddings**2, 1, keepdims=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def reset_drnn(model_name='relsingle', bi=True, dep_embed_size=25, pos_embed_size=25, \n",
    "               word_embed_size=None, max_grad_norm=3., max_to_keep=0, hidden_size=300,\n",
    "               supervised=True, interactive=True):\n",
    "    if word_embed_size:    \n",
    "        config = {\n",
    "            'max_num_steps':DH.max_seq_len,\n",
    "            'word_embed_size':word_embed_size,\n",
    "            'dep_embed_size':dep_embed_size,\n",
    "            'pos_embed_size':pos_embed_size,\n",
    "            'hidden_size':hidden_size,\n",
    "            'bidirectional':bi,\n",
    "            'supervised':supervised,\n",
    "            'interactive':interactive,\n",
    "            'hidden_layer_size':1000,\n",
    "            'vocab_size':DH.vocab_size,\n",
    "            'dep_vocab_size':DH.dep_size,\n",
    "            'pos_vocab_size':DH.pos_size,\n",
    "            'num_predict_classes':num_classes,\n",
    "            'pretrained_word_embeddings':None,\n",
    "            'max_grad_norm':3.,\n",
    "            'model_name':model_name,\n",
    "            'max_to_keep':max_to_keep,\n",
    "            'checkpoint_prefix':'checkpoints/',\n",
    "            'summary_prefix':'tensor_summaries/'\n",
    "        }\n",
    "    else: # use pretrained google vectors\n",
    "        config = {\n",
    "            'max_num_steps':DH.max_seq_len,\n",
    "            'word_embed_size':300,\n",
    "            'dep_embed_size':dep_embed_size,\n",
    "            'pos_embed_size':pos_embed_size,\n",
    "            'hidden_size':hidden_size,\n",
    "            'bidirectional':bi,\n",
    "            'supervised':supervised,\n",
    "            'interactive':interactive,\n",
    "            'hidden_layer_size':1000,\n",
    "            'vocab_size':DH.vocab_size,\n",
    "            'dep_vocab_size':DH.dep_size,\n",
    "            'pos_vocab_size':DH.pos_size,\n",
    "            'num_predict_classes':num_classes,\n",
    "            'pretrained_word_embeddings':word_embeddings,\n",
    "            'max_grad_norm':3.,\n",
    "            'model_name':model_name,            \n",
    "            'max_to_keep':max_to_keep,\n",
    "            'checkpoint_prefix':'checkpoints/',\n",
    "            'summary_prefix':'tensor_summaries/'\n",
    "        }\n",
    "    try:\n",
    "        tf.reset_default_graph()\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        tf.get_default_session().close()\n",
    "    except:\n",
    "        pass\n",
    "    drnn = nn.RelEmbed(config)\n",
    "    print(drnn)\n",
    "    return drnn\n",
    "# drnn = reset_drnn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_validation_test(num_nearby=20):\n",
    "    # TODO: Pass x_or_y to validation_phrase\n",
    "    valid_phrases, valid_targets , _, valid_lens, _ = DH.validation_batch()\n",
    "#     print(\"V phrase shape\", valid_phrases.shape)\n",
    "    random_index = int(random.uniform(0, len(valid_lens)))\n",
    "    query_phrase = valid_phrases[random_index]\n",
    "    query_len = valid_lens[random_index]\n",
    "    query_target = valid_targets[random_index].reshape((1,-1))\n",
    "    padded_qp = np.zeros([DH.max_seq_len, 3]).astype(np.int32)\n",
    "    padded_qp[:len(query_phrase), 0] = [x[0] for x in query_phrase]\n",
    "    padded_qp[:len(query_phrase), 1] = [x[1] for x in query_phrase]\n",
    "    padded_qp[:len(query_phrase), 2] = [x[2] for x in query_phrase] \n",
    "    dists, phrase_idx = drnn.validation_phrase_nearby(padded_qp, query_len, query_target,\n",
    "                                                      valid_phrases, valid_lens, valid_targets)\n",
    "    print(\"=\"*80)\n",
    "    print(\"Top %i/%i closest phrases to '%s' <%s>\" \n",
    "          % (num_nearby, DH.valid_size(),\n",
    "             DH.sequence_to_sentence(query_phrase, query_len), \n",
    "             DH.vocab_at(query_target[0,0])))\n",
    "    for i in range(num_nearby):\n",
    "        dist = dists[i]\n",
    "        phrase = valid_phrases[phrase_idx[i]]\n",
    "        len_ = valid_lens[phrase_idx[i]]\n",
    "        target = valid_targets[phrase_idx[i]]\n",
    "        print(\"%i: %0.3f :'%s' <%s>\" \n",
    "              % (i, dist, \n",
    "                 DH.sequence_to_sentence(phrase, len_),\n",
    "                 DH.vocab_at(target[0])))\n",
    "    print(\"=\"*80)\n",
    "#     drnn.save_validation_accuracy(frac_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def time_left(num_epochs, num_steps, fit_time, nearby_time, start_time, nearby_mod):\n",
    "    total = num_epochs*num_steps*fit_time + ((num_epochs*num_steps)/float(nearby_mod))*nearby_time\n",
    "    return total - (time() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 300)\n",
      "13 13\n",
      "<DPNN: W:300, D:25, P:25 H:300, V:27070>\n",
      "(0:0:4) step 0/18666, epoch 0 Training Loss = 0.71364 :: 371.879 phrases/sec\n",
      "================================================================================\n",
      "Top 25/4714 closest phrases to 'purse offers <X>' <it>\n",
      "0: 1.000 :'purse offers <X>' <it>\n",
      "1: 0.900 :'discounts offer <X>' <traders>\n",
      "2: 0.892 :'stays offered <X>' <hill>\n",
      "3: 0.891 :'tutors provides <X>' <it>\n",
      "4: 0.890 :'use offered <X>' <council>\n",
      "5: 0.883 :'hook to offers <X>' <service>\n",
      "6: 0.869 :'semen urine for exit provides <X>' <urethra>\n",
      "7: 0.868 :'ancestors worship place gives <X>' <temple>\n",
      "8: 0.865 :'award is <X>' <award>\n",
      "9: 0.864 :'bytes is <X>' <size>\n",
      "10: 0.862 :'her holds <X>' <he>\n",
      "11: 0.862 :'access provides route <X>' <branches>\n",
      "12: 0.861 :'seat won is <X>' <it>\n",
      "13: 0.860 :'style features <X>' <it>\n",
      "14: 0.859 :'clues provide <X>' <notes>\n",
      "15: 0.858 :'name receives <X>' <bridge>\n",
      "16: 0.856 :'woman bring <X>' <police>\n",
      "17: 0.856 :'place takes <X>' <festival>\n",
      "18: 0.854 :'is <X>' <km2>\n",
      "19: 0.854 :'woman is <X>' <she>\n",
      "20: 0.852 :'series is <X>' <brain>\n",
      "21: 0.849 :'vision has holds <X>' <he>\n",
      "22: 0.847 :'passages for <X>' <strength>\n",
      "23: 0.847 :'ion is <X>' <example>\n",
      "24: 0.847 :'defence of form is <X>' <force>\n",
      "================================================================================\n",
      "Validation loss: -0.1244\n",
      "(0:2:0) step 10/18666, epoch 0 Training Loss = 0.62677 :: 242.905 phrases/sec\n",
      "(0:2:40) step 20/18666, epoch 0 Training Loss = 0.61102 :: 243.225 phrases/sec\n",
      "(0:2:59) step 30/18666, epoch 0 Training Loss = 0.61364 :: 230.713 phrases/sec\n",
      "(0:3:13) step 40/18666, epoch 0 Training Loss = 0.61650 :: 232.264 phrases/sec\n",
      "(0:3:27) step 50/18666, epoch 0 Training Loss = 0.61161 :: 233.082 phrases/sec\n",
      "(0:3:43) step 60/18666, epoch 0 Training Loss = 0.60341 :: 232.045 phrases/sec\n",
      "(0:4:8) step 70/18666, epoch 0 Training Loss = 0.61212 :: 232.194 phrases/sec\n",
      "(0:4:28) step 80/18666, epoch 0 Training Loss = 0.61572 :: 231.857 phrases/sec\n",
      "(0:4:47) step 90/18666, epoch 0 Training Loss = 0.61804 :: 232.169 phrases/sec\n",
      "(0:5:11) step 100/18666, epoch 0 Training Loss = 0.61567 :: 231.370 phrases/sec\n",
      "(0:5:27) step 110/18666, epoch 0 Training Loss = 0.61008 :: 231.565 phrases/sec\n",
      "(0:5:41) step 120/18666, epoch 0 Training Loss = 0.60535 :: 233.059 phrases/sec\n",
      "(0:5:55) step 130/18666, epoch 0 Training Loss = 0.60513 :: 233.977 phrases/sec\n",
      "(0:6:8) step 140/18666, epoch 0 Training Loss = 0.60747 :: 234.742 phrases/sec\n",
      "(0:6:21) step 150/18666, epoch 0 Training Loss = 0.61325 :: 236.146 phrases/sec\n",
      "(0:6:34) step 160/18666, epoch 0 Training Loss = 0.60795 :: 236.429 phrases/sec\n",
      "(0:6:48) step 170/18666, epoch 0 Training Loss = 0.61253 :: 237.130 phrases/sec\n",
      "(0:7:1) step 180/18666, epoch 0 Training Loss = 0.60757 :: 237.399 phrases/sec\n",
      "(0:7:15) step 190/18666, epoch 0 Training Loss = 0.60683 :: 238.000 phrases/sec\n",
      "(0:7:28) step 200/18666, epoch 0 Training Loss = 0.60468 :: 238.210 phrases/sec\n",
      "(0:7:41) step 210/18666, epoch 0 Training Loss = 0.59449 :: 238.662 phrases/sec\n",
      "(0:7:55) step 220/18666, epoch 0 Training Loss = 0.61065 :: 238.852 phrases/sec\n",
      "(0:8:8) step 230/18666, epoch 0 Training Loss = 0.61182 :: 239.271 phrases/sec\n",
      "(0:8:22) step 240/18666, epoch 0 Training Loss = 0.60147 :: 239.220 phrases/sec\n",
      "(0:8:35) step 250/18666, epoch 0 Training Loss = 0.60923 :: 239.489 phrases/sec\n",
      "(0:8:49) step 260/18666, epoch 0 Training Loss = 0.60603 :: 239.274 phrases/sec\n",
      "(0:9:2) step 270/18666, epoch 0 Training Loss = 0.60170 :: 239.589 phrases/sec\n",
      "(0:9:15) step 280/18666, epoch 0 Training Loss = 0.59256 :: 239.897 phrases/sec\n",
      "(0:9:28) step 290/18666, epoch 0 Training Loss = 0.58949 :: 240.108 phrases/sec\n",
      "(0:9:41) step 300/18666, epoch 0 Training Loss = 0.61204 :: 240.270 phrases/sec\n",
      "(0:9:55) step 310/18666, epoch 0 Training Loss = 0.58792 :: 240.418 phrases/sec\n",
      "(0:10:9) step 320/18666, epoch 0 Training Loss = 0.59058 :: 240.073 phrases/sec\n",
      "(0:10:22) step 330/18666, epoch 0 Training Loss = 0.59289 :: 240.325 phrases/sec\n",
      "(0:10:36) step 340/18666, epoch 0 Training Loss = 0.58857 :: 239.922 phrases/sec\n",
      "(0:10:50) step 350/18666, epoch 0 Training Loss = 0.59497 :: 240.025 phrases/sec\n",
      "(0:11:2) step 360/18666, epoch 0 Training Loss = 0.59946 :: 240.402 phrases/sec\n",
      "(0:11:16) step 370/18666, epoch 0 Training Loss = 0.60290 :: 240.342 phrases/sec\n",
      "(0:11:30) step 380/18666, epoch 0 Training Loss = 0.59821 :: 239.849 phrases/sec\n",
      "(0:11:45) step 390/18666, epoch 0 Training Loss = 0.59758 :: 239.439 phrases/sec\n",
      "(0:11:58) step 400/18666, epoch 0 Training Loss = 0.59921 :: 239.634 phrases/sec\n",
      "(0:12:11) step 410/18666, epoch 0 Training Loss = 0.59916 :: 239.875 phrases/sec\n",
      "(0:12:25) step 420/18666, epoch 0 Training Loss = 0.58577 :: 239.748 phrases/sec\n",
      "(0:12:38) step 430/18666, epoch 0 Training Loss = 0.58487 :: 239.751 phrases/sec\n",
      "(0:12:51) step 440/18666, epoch 0 Training Loss = 0.60467 :: 239.863 phrases/sec\n",
      "(0:13:4) step 450/18666, epoch 0 Training Loss = 0.60649 :: 240.024 phrases/sec\n",
      "(0:13:18) step 460/18666, epoch 0 Training Loss = 0.58959 :: 239.886 phrases/sec\n",
      "(0:13:32) step 470/18666, epoch 0 Training Loss = 0.60131 :: 239.829 phrases/sec\n",
      "(0:13:45) step 480/18666, epoch 0 Training Loss = 0.59484 :: 240.098 phrases/sec\n",
      "(0:13:58) step 490/18666, epoch 0 Training Loss = 0.60430 :: 240.128 phrases/sec\n",
      "(0:14:12) step 500/18666, epoch 0 Training Loss = 0.59136 :: 240.146 phrases/sec\n",
      "================================================================================\n",
      "Top 25/4714 closest phrases to 'population of majority in promotion until served <X>' <he>\n",
      "0: 1.000 :'population of majority in promotion until served <X>' <he>\n",
      "1: 0.998 :'population to is <X>' <home>\n",
      "2: 0.998 :'population spread in <X>' <town>\n",
      "3: 0.997 :'population had in <Y>' <century>\n",
      "4: 0.997 :'population has at <Y>' <census>\n",
      "5: 0.997 :'population from <X>' <program>\n",
      "6: 0.996 :'population has <X>' <municipality>\n",
      "7: 0.996 :'population consisted by end of <X>' <century>\n",
      "8: 0.996 :'population of <Y>' <people>\n",
      "9: 0.994 :'population was at <Y>' <census>\n",
      "10: 0.965 :'we relate group in <Y>' <ways>\n",
      "11: 0.964 :'we know cites <X>' <he>\n",
      "12: 0.963 :'we accept have <Y>' <we>\n",
      "13: 0.958 :'veterans of <X>' <needs>\n",
      "14: 0.950 :'areas buildings jurisdiction has <X>' <administration>\n",
      "15: 0.950 :'militants undergone training included <Y>' <warfare>\n",
      "16: 0.950 :'numbers in found found <Y>' <it>\n",
      "17: 0.950 :'incarceration relocation of issues <OOV> <X>' <activists>\n",
      "18: 0.949 :'chinatown throughout scattering <X>' <recipes>\n",
      "19: 0.949 :'constellation of <Y>' <satellites>\n",
      "20: 0.949 :'british of <X>' <rest>\n",
      "21: 0.948 :'one be <Y>' <years>\n",
      "22: 0.948 :'crucible with began <X>' <use>\n",
      "23: 0.948 :'areas organise for <Y>' <members>\n",
      "24: 0.948 :'states mexico of <X>' <park>\n",
      "================================================================================\n",
      "Validation loss: -0.5068\n",
      "(0:16:11) step 510/18666, epoch 0 Training Loss = 0.59760 :: 239.607 phrases/sec\n",
      "(0:16:29) step 520/18666, epoch 0 Training Loss = 0.58300 :: 239.171 phrases/sec\n",
      "(0:16:44) step 530/18666, epoch 0 Training Loss = 0.58531 :: 239.215 phrases/sec\n",
      "(0:17:0) step 540/18666, epoch 0 Training Loss = 0.59306 :: 239.270 phrases/sec\n",
      "(0:17:15) step 550/18666, epoch 0 Training Loss = 0.58819 :: 239.246 phrases/sec\n",
      "(0:17:32) step 560/18666, epoch 0 Training Loss = 0.58300 :: 239.206 phrases/sec\n",
      "(0:17:46) step 570/18666, epoch 0 Training Loss = 0.59870 :: 239.419 phrases/sec\n",
      "(0:18:0) step 580/18666, epoch 0 Training Loss = 0.59567 :: 239.534 phrases/sec\n",
      "(0:18:14) step 590/18666, epoch 0 Training Loss = 0.59387 :: 239.545 phrases/sec\n",
      "(0:18:28) step 600/18666, epoch 0 Training Loss = 0.59945 :: 239.535 phrases/sec\n",
      "(0:18:41) step 610/18666, epoch 0 Training Loss = 0.59830 :: 239.739 phrases/sec\n",
      "(0:18:54) step 620/18666, epoch 0 Training Loss = 0.59471 :: 239.819 phrases/sec\n",
      "(0:19:8) step 630/18666, epoch 0 Training Loss = 0.59711 :: 239.681 phrases/sec\n",
      "(0:19:21) step 640/18666, epoch 0 Training Loss = 0.60118 :: 239.844 phrases/sec\n",
      "(0:19:34) step 650/18666, epoch 0 Training Loss = 0.58803 :: 240.023 phrases/sec\n",
      "(0:19:48) step 660/18666, epoch 0 Training Loss = 0.58788 :: 240.066 phrases/sec\n",
      "(0:20:1) step 670/18666, epoch 0 Training Loss = 0.59759 :: 240.147 phrases/sec\n",
      "(0:20:14) step 680/18666, epoch 0 Training Loss = 0.58222 :: 240.205 phrases/sec\n",
      "(0:20:27) step 690/18666, epoch 0 Training Loss = 0.59040 :: 240.232 phrases/sec\n",
      "(0:20:41) step 700/18666, epoch 0 Training Loss = 0.59282 :: 240.260 phrases/sec\n",
      "(0:20:56) step 710/18666, epoch 0 Training Loss = 0.58366 :: 239.693 phrases/sec\n",
      "(0:21:12) step 720/18666, epoch 0 Training Loss = 0.59727 :: 239.036 phrases/sec\n",
      "(0:21:26) step 730/18666, epoch 0 Training Loss = 0.59355 :: 238.902 phrases/sec\n",
      "(0:21:41) step 740/18666, epoch 0 Training Loss = 0.58925 :: 238.628 phrases/sec\n",
      "(0:21:56) step 750/18666, epoch 0 Training Loss = 0.57974 :: 238.305 phrases/sec\n",
      "(0:22:9) step 760/18666, epoch 0 Training Loss = 0.59544 :: 238.238 phrases/sec\n",
      "(0:22:23) step 770/18666, epoch 0 Training Loss = 0.59769 :: 238.134 phrases/sec\n",
      "(0:22:37) step 780/18666, epoch 0 Training Loss = 0.57910 :: 238.214 phrases/sec\n",
      "(0:22:51) step 790/18666, epoch 0 Training Loss = 0.59425 :: 238.083 phrases/sec\n",
      "(0:23:3) step 800/18666, epoch 0 Training Loss = 0.58883 :: 238.250 phrases/sec\n",
      "(0:23:16) step 810/18666, epoch 0 Training Loss = 0.58745 :: 238.427 phrases/sec\n",
      "(0:23:29) step 820/18666, epoch 0 Training Loss = 0.60215 :: 238.596 phrases/sec\n",
      "(0:23:43) step 830/18666, epoch 0 Training Loss = 0.59760 :: 238.451 phrases/sec\n",
      "(0:23:58) step 840/18666, epoch 0 Training Loss = 0.59450 :: 238.202 phrases/sec\n",
      "(0:24:11) step 850/18666, epoch 0 Training Loss = 0.59410 :: 238.351 phrases/sec\n",
      "(0:24:26) step 860/18666, epoch 0 Training Loss = 0.59071 :: 238.133 phrases/sec\n",
      "(0:24:39) step 870/18666, epoch 0 Training Loss = 0.59176 :: 238.075 phrases/sec\n",
      "(0:24:52) step 880/18666, epoch 0 Training Loss = 0.59806 :: 238.221 phrases/sec\n",
      "(0:25:5) step 890/18666, epoch 0 Training Loss = 0.59564 :: 238.380 phrases/sec\n",
      "(0:25:19) step 900/18666, epoch 0 Training Loss = 0.58504 :: 238.473 phrases/sec\n",
      "(0:25:33) step 910/18666, epoch 0 Training Loss = 0.60074 :: 238.370 phrases/sec\n",
      "(0:25:46) step 920/18666, epoch 0 Training Loss = 0.58255 :: 238.478 phrases/sec\n",
      "(0:25:59) step 930/18666, epoch 0 Training Loss = 0.58632 :: 238.530 phrases/sec\n",
      "(0:26:13) step 940/18666, epoch 0 Training Loss = 0.57894 :: 238.465 phrases/sec\n",
      "(0:26:26) step 950/18666, epoch 0 Training Loss = 0.60095 :: 238.600 phrases/sec\n",
      "(0:26:39) step 960/18666, epoch 0 Training Loss = 0.59354 :: 238.659 phrases/sec\n",
      "(0:26:53) step 970/18666, epoch 0 Training Loss = 0.58865 :: 238.719 phrases/sec\n",
      "(0:27:6) step 980/18666, epoch 0 Training Loss = 0.61003 :: 238.840 phrases/sec\n",
      "(0:27:21) step 990/18666, epoch 0 Training Loss = 0.59976 :: 238.578 phrases/sec\n",
      "(0:27:35) step 1000/18666, epoch 0 Training Loss = 0.59900 :: 238.431 phrases/sec\n",
      "================================================================================\n",
      "Top 25/4714 closest phrases to 'battle of plains of <Y>' <abraham>\n",
      "0: 1.000 :'battle of plains of <Y>' <abraham>\n",
      "1: 0.998 :'battle of <X>' <site>\n",
      "2: 0.998 :'battle of <X>' <instructions>\n",
      "3: 0.997 :'battle after attended in <X>' <abbey>\n",
      "4: 0.996 :'battle in engaged <X>' <who>\n",
      "5: 0.996 :'battle in <Y>' <ce>\n",
      "6: 0.996 :'battle at <X>' <victory>\n",
      "7: 0.904 :'dam is dam on island of <Y>' <borneo>\n",
      "8: 0.902 :'japan withdrew bringing <Y>' <it>\n",
      "9: 0.899 :'japan <OOV> provided <X>' <support>\n",
      "10: 0.898 :'japan between <X>' <contention>\n",
      "11: 0.897 :'member as announced on <X>' <march>\n",
      "12: 0.897 :'turrets were were <Y>' <they>\n",
      "13: 0.897 :'charles is <Y>' <variety>\n",
      "14: 0.896 :'dam on <Y>' <island>\n",
      "15: 0.896 :'mccarthy was member of <Y>' <team>\n",
      "16: 0.896 :'member as elected on <X>' <attempt>\n",
      "17: 0.895 :'intention was combine <Y>' <am>\n",
      "18: 0.895 :'battle went for <Y>' <stephen>\n",
      "19: 0.895 :'cheshire in born <X>' <cole>\n",
      "20: 0.894 :'member as is committee with <X>' <head>\n",
      "21: 0.894 :'member as elected <X>' <he>\n",
      "22: 0.893 :'tram of <X>' <extension>\n",
      "23: 0.893 :'india from is throughout <X>' <area>\n",
      "24: 0.893 :'debut in seconds of <X>' <action>\n",
      "================================================================================\n",
      "Validation loss: -0.5001\n",
      "(0:29:26) step 1010/18666, epoch 0 Training Loss = 0.59029 :: 238.332 phrases/sec\n",
      "(0:29:47) step 1020/18666, epoch 0 Training Loss = 0.59448 :: 238.338 phrases/sec\n",
      "Saving model...\n",
      "Saving model to file: checkpoints/wikigru_50k.ckpt-1029-0\n",
      "(0:30:48) step 1030/18666, epoch 0 Training Loss = 0.59263 :: 238.423 phrases/sec\n",
      "(0:31:3) step 1040/18666, epoch 0 Training Loss = 0.59527 :: 238.444 phrases/sec\n",
      "(0:31:17) step 1050/18666, epoch 0 Training Loss = 0.60981 :: 238.513 phrases/sec\n",
      "(0:31:31) step 1060/18666, epoch 0 Training Loss = 0.59119 :: 238.420 phrases/sec\n",
      "(0:31:45) step 1070/18666, epoch 0 Training Loss = 0.59141 :: 238.450 phrases/sec\n",
      "(0:31:58) step 1080/18666, epoch 0 Training Loss = 0.58729 :: 238.560 phrases/sec\n",
      "(0:32:11) step 1090/18666, epoch 0 Training Loss = 0.57728 :: 238.662 phrases/sec\n",
      "(0:32:24) step 1100/18666, epoch 0 Training Loss = 0.59249 :: 238.748 phrases/sec\n",
      "(0:32:37) step 1110/18666, epoch 0 Training Loss = 0.59883 :: 238.891 phrases/sec\n",
      "(0:32:50) step 1120/18666, epoch 0 Training Loss = 0.59347 :: 238.965 phrases/sec\n",
      "(0:33:4) step 1130/18666, epoch 0 Training Loss = 0.59401 :: 238.935 phrases/sec\n",
      "(0:33:18) step 1140/18666, epoch 0 Training Loss = 0.58864 :: 238.949 phrases/sec\n",
      "(0:33:31) step 1150/18666, epoch 0 Training Loss = 0.58609 :: 238.947 phrases/sec\n",
      "(0:33:45) step 1160/18666, epoch 0 Training Loss = 0.58777 :: 238.981 phrases/sec\n",
      "(0:33:58) step 1170/18666, epoch 0 Training Loss = 0.58849 :: 239.077 phrases/sec\n",
      "(0:34:11) step 1180/18666, epoch 0 Training Loss = 0.58639 :: 239.126 phrases/sec\n",
      "(0:34:24) step 1190/18666, epoch 0 Training Loss = 0.59939 :: 239.171 phrases/sec\n",
      "(0:34:38) step 1200/18666, epoch 0 Training Loss = 0.60898 :: 239.193 phrases/sec\n",
      "(0:34:51) step 1210/18666, epoch 0 Training Loss = 0.58990 :: 239.230 phrases/sec\n",
      "(0:35:6) step 1220/18666, epoch 0 Training Loss = 0.58792 :: 239.093 phrases/sec\n",
      "(0:35:21) step 1230/18666, epoch 0 Training Loss = 0.59365 :: 238.851 phrases/sec\n",
      "(0:35:34) step 1240/18666, epoch 0 Training Loss = 0.59106 :: 238.866 phrases/sec\n",
      "(0:35:48) step 1250/18666, epoch 0 Training Loss = 0.59625 :: 238.908 phrases/sec\n",
      "(0:36:1) step 1260/18666, epoch 0 Training Loss = 0.59389 :: 238.898 phrases/sec\n",
      "(0:36:14) step 1270/18666, epoch 0 Training Loss = 0.59340 :: 238.993 phrases/sec\n",
      "(0:36:27) step 1280/18666, epoch 0 Training Loss = 0.60197 :: 239.072 phrases/sec\n",
      "(0:36:40) step 1290/18666, epoch 0 Training Loss = 0.58353 :: 239.156 phrases/sec\n",
      "(0:36:53) step 1300/18666, epoch 0 Training Loss = 0.58462 :: 239.236 phrases/sec\n",
      "(0:37:6) step 1310/18666, epoch 0 Training Loss = 0.60644 :: 239.329 phrases/sec\n",
      "(0:37:19) step 1320/18666, epoch 0 Training Loss = 0.58305 :: 239.400 phrases/sec\n",
      "(0:37:32) step 1330/18666, epoch 0 Training Loss = 0.59836 :: 239.493 phrases/sec\n",
      "(0:37:45) step 1340/18666, epoch 0 Training Loss = 0.58638 :: 239.577 phrases/sec\n",
      "(0:37:58) step 1350/18666, epoch 0 Training Loss = 0.58694 :: 239.654 phrases/sec\n",
      "(0:38:12) step 1360/18666, epoch 0 Training Loss = 0.58848 :: 239.706 phrases/sec\n",
      "(0:38:25) step 1370/18666, epoch 0 Training Loss = 0.59716 :: 239.770 phrases/sec\n",
      "(0:38:38) step 1380/18666, epoch 0 Training Loss = 0.58959 :: 239.856 phrases/sec\n",
      "(0:38:51) step 1390/18666, epoch 0 Training Loss = 0.59185 :: 239.932 phrases/sec\n",
      "(0:39:4) step 1400/18666, epoch 0 Training Loss = 0.59468 :: 239.960 phrases/sec\n",
      "(0:39:17) step 1410/18666, epoch 0 Training Loss = 0.60051 :: 240.039 phrases/sec\n",
      "(0:39:30) step 1420/18666, epoch 0 Training Loss = 0.58699 :: 240.061 phrases/sec\n",
      "(0:39:45) step 1430/18666, epoch 0 Training Loss = 0.60016 :: 239.904 phrases/sec\n",
      "(0:39:59) step 1440/18666, epoch 0 Training Loss = 0.59220 :: 239.823 phrases/sec\n",
      "(0:40:14) step 1450/18666, epoch 0 Training Loss = 0.58967 :: 239.659 phrases/sec\n",
      "(0:40:28) step 1460/18666, epoch 0 Training Loss = 0.59673 :: 239.598 phrases/sec\n",
      "(0:40:41) step 1470/18666, epoch 0 Training Loss = 0.60352 :: 239.630 phrases/sec\n",
      "(0:40:54) step 1480/18666, epoch 0 Training Loss = 0.58912 :: 239.648 phrases/sec\n",
      "(0:41:8) step 1490/18666, epoch 0 Training Loss = 0.59559 :: 239.694 phrases/sec\n",
      "(0:41:21) step 1500/18666, epoch 0 Training Loss = 0.59543 :: 239.735 phrases/sec\n",
      "================================================================================\n",
      "Top 25/4714 closest phrases to 'percent of <Y>' <respondents>\n",
      "0: 1.000 :'percent of <Y>' <respondents>\n",
      "1: 0.989 :'percent in <Y>' <state>\n",
      "2: 0.988 :'percent by grew product per <X>' <capita>\n",
      "3: 0.924 :'movement is deformation of <Y>' <plate>\n",
      "4: 0.921 :'company made into <Y>' <trade>\n",
      "5: 0.920 :'movement of <Y>' <awareness>\n",
      "6: 0.919 :'company is lay <Y>' <gauge>\n",
      "7: 0.915 :'bruce died succeeded in <Y>' <barony>\n",
      "8: 0.915 :'around is <Y>' <cartoon>\n",
      "9: 0.914 :'chick is paler <OOV> than <Y>' <adult>\n",
      "10: 0.914 :'company crossed <Y>' <rhine>\n",
      "11: 0.914 :'company dragged into <Y>' <lawsuit>\n",
      "12: 0.912 :'degree of <Y>' <latitude>\n",
      "13: 0.912 :'degree of <Y>' <effect>\n",
      "14: 0.912 :'company operated for <X>' <period>\n",
      "15: 0.912 :'one be <Y>' <years>\n",
      "16: 0.911 :'movement led to <Y>' <establishment>\n",
      "17: 0.911 :'company merged with <Y>' <ltd>\n",
      "18: 0.911 :'organizers chose include <Y>' <days>\n",
      "19: 0.910 :'joining serves as <Y>' <place>\n",
      "20: 0.910 :'company invested reported in <X>' <april>\n",
      "21: 0.910 :'part of <Y>' <problem>\n",
      "22: 0.909 :'company formed opened at <Y>' <bridge>\n",
      "23: 0.909 :'jack sleeping with <Y>' <girlfriend>\n",
      "24: 0.909 :'school for blind in <Y>' <liverpool>\n",
      "================================================================================\n",
      "Validation loss: -0.4976\n",
      "(0:43:44) step 1510/18666, epoch 0 Training Loss = 0.59523 :: 239.767 phrases/sec\n",
      "(0:44:4) step 1520/18666, epoch 0 Training Loss = 0.59701 :: 239.800 phrases/sec\n",
      "(0:44:26) step 1530/18666, epoch 0 Training Loss = 0.59228 :: 239.649 phrases/sec\n",
      "(0:44:40) step 1540/18666, epoch 0 Training Loss = 0.58953 :: 239.701 phrases/sec\n",
      "(0:44:54) step 1550/18666, epoch 0 Training Loss = 0.58966 :: 239.759 phrases/sec\n",
      "(0:45:8) step 1560/18666, epoch 0 Training Loss = 0.59502 :: 239.831 phrases/sec\n",
      "(0:45:22) step 1570/18666, epoch 0 Training Loss = 0.58786 :: 239.750 phrases/sec\n",
      "(0:45:36) step 1580/18666, epoch 0 Training Loss = 0.57845 :: 239.783 phrases/sec\n",
      "(0:45:50) step 1590/18666, epoch 0 Training Loss = 0.58793 :: 239.801 phrases/sec\n",
      "(0:46:3) step 1600/18666, epoch 0 Training Loss = 0.60007 :: 239.857 phrases/sec\n",
      "(0:46:16) step 1610/18666, epoch 0 Training Loss = 0.58808 :: 239.894 phrases/sec\n",
      "(0:46:30) step 1620/18666, epoch 0 Training Loss = 0.60651 :: 239.868 phrases/sec\n",
      "(0:46:44) step 1630/18666, epoch 0 Training Loss = 0.59121 :: 239.888 phrases/sec\n",
      "(0:46:57) step 1640/18666, epoch 0 Training Loss = 0.58632 :: 239.976 phrases/sec\n",
      "(0:47:10) step 1650/18666, epoch 0 Training Loss = 0.58677 :: 240.041 phrases/sec\n",
      "(0:47:23) step 1660/18666, epoch 0 Training Loss = 0.59638 :: 240.109 phrases/sec\n",
      "(0:47:36) step 1670/18666, epoch 0 Training Loss = 0.58838 :: 240.176 phrases/sec\n",
      "(0:47:49) step 1680/18666, epoch 0 Training Loss = 0.58785 :: 240.261 phrases/sec\n",
      "(0:48:2) step 1690/18666, epoch 0 Training Loss = 0.59131 :: 240.318 phrases/sec\n",
      "(0:48:15) step 1700/18666, epoch 0 Training Loss = 0.58844 :: 240.381 phrases/sec\n",
      "(0:48:28) step 1710/18666, epoch 0 Training Loss = 0.58483 :: 240.444 phrases/sec\n",
      "(0:48:41) step 1720/18666, epoch 0 Training Loss = 0.58917 :: 240.478 phrases/sec\n",
      "(0:48:55) step 1730/18666, epoch 0 Training Loss = 0.58913 :: 240.443 phrases/sec\n",
      "(0:49:8) step 1740/18666, epoch 0 Training Loss = 0.59188 :: 240.514 phrases/sec\n",
      "(0:49:21) step 1750/18666, epoch 0 Training Loss = 0.58979 :: 240.564 phrases/sec\n",
      "(0:49:34) step 1760/18666, epoch 0 Training Loss = 0.58558 :: 240.601 phrases/sec\n",
      "(0:49:47) step 1770/18666, epoch 0 Training Loss = 0.60315 :: 240.675 phrases/sec\n",
      "(0:50:1) step 1780/18666, epoch 0 Training Loss = 0.58835 :: 240.653 phrases/sec\n",
      "(0:50:14) step 1790/18666, epoch 0 Training Loss = 0.57960 :: 240.668 phrases/sec\n",
      "(0:50:27) step 1800/18666, epoch 0 Training Loss = 0.58901 :: 240.737 phrases/sec\n",
      "(0:50:40) step 1810/18666, epoch 0 Training Loss = 0.58523 :: 240.804 phrases/sec\n",
      "(0:50:53) step 1820/18666, epoch 0 Training Loss = 0.59256 :: 240.836 phrases/sec\n",
      "(0:51:7) step 1830/18666, epoch 0 Training Loss = 0.58776 :: 240.850 phrases/sec\n",
      "(0:51:20) step 1840/18666, epoch 0 Training Loss = 0.58962 :: 240.850 phrases/sec\n",
      "(0:51:33) step 1850/18666, epoch 0 Training Loss = 0.60483 :: 240.874 phrases/sec\n",
      "(0:51:47) step 1860/18666, epoch 0 Training Loss = 0.58774 :: 240.898 phrases/sec\n",
      "(0:52:0) step 1870/18666, epoch 0 Training Loss = 0.59713 :: 240.942 phrases/sec\n",
      "(0:52:13) step 1880/18666, epoch 0 Training Loss = 0.60205 :: 240.914 phrases/sec\n",
      "(0:52:27) step 1890/18666, epoch 0 Training Loss = 0.59903 :: 240.925 phrases/sec\n",
      "(0:52:41) step 1900/18666, epoch 0 Training Loss = 0.59739 :: 240.893 phrases/sec\n",
      "(0:52:54) step 1910/18666, epoch 0 Training Loss = 0.59265 :: 240.871 phrases/sec\n",
      "(0:53:7) step 1920/18666, epoch 0 Training Loss = 0.59270 :: 240.943 phrases/sec\n",
      "(0:53:21) step 1930/18666, epoch 0 Training Loss = 0.60183 :: 240.937 phrases/sec\n",
      "(0:53:34) step 1940/18666, epoch 0 Training Loss = 0.59322 :: 240.916 phrases/sec\n",
      "(0:53:47) step 1950/18666, epoch 0 Training Loss = 0.59672 :: 240.982 phrases/sec\n",
      "(0:54:1) step 1960/18666, epoch 0 Training Loss = 0.59325 :: 240.984 phrases/sec\n",
      "(0:54:14) step 1970/18666, epoch 0 Training Loss = 0.59417 :: 241.004 phrases/sec\n",
      "(0:54:28) step 1980/18666, epoch 0 Training Loss = 0.58332 :: 240.990 phrases/sec\n",
      "(0:54:40) step 1990/18666, epoch 0 Training Loss = 0.59875 :: 241.039 phrases/sec\n",
      "(0:54:53) step 2000/18666, epoch 0 Training Loss = 0.60016 :: 241.098 phrases/sec\n",
      "================================================================================\n",
      "Top 25/4714 closest phrases to 'records of race organized by <Y>' <club>\n",
      "0: 1.000 :'records of race organized by <Y>' <club>\n",
      "1: 0.972 :'records under released <X>' <album>\n",
      "2: 0.971 :'records limit on released on <X>' <december>\n",
      "3: 0.965 :'records files to <X>' <access>\n",
      "4: 0.948 :'river of mouth at located in <X>' <municipality>\n",
      "5: 0.946 :'river of depth become due <OOV> from <Y>' <farming>\n",
      "6: 0.943 :'people of <NUM> as listed <X>' <jacobs>\n",
      "7: 0.941 :'him by conquered <X>' <territories>\n",
      "8: 0.940 :'university at department of <X>' <chair>\n",
      "9: 0.939 :'court on found distance from <Y>' <road>\n",
      "10: 0.939 :'university shown result of encounters between kinds of <Y>' <result>\n",
      "11: 0.936 :'may by chosen <X>' <topics>\n",
      "12: 0.936 :'states in university of <X>' <york>\n",
      "13: 0.935 :'county of northeast in <X>' <area>\n",
      "14: 0.934 :'york of university from received in <X>' <mathematics>\n",
      "15: 0.934 :'recycling of benefits warming discusses <X>' <snow>\n",
      "16: 0.934 :'him to subject of <X>' <importance>\n",
      "17: 0.934 :'workers of group marched in <Y>' <protest>\n",
      "18: 0.934 :'he was president of council of <Y>' <scouts>\n",
      "19: 0.934 :'university from graduated born in <X>' <district>\n",
      "20: 0.933 :'university from gained graduated from <X>' <university>\n",
      "21: 0.933 :'party of member get cover <Y>' <lakh>\n",
      "22: 0.933 :'he is president member of <Y>' <institute>\n",
      "23: 0.933 :'part as contribute meet needs of <X>' <groups>\n",
      "24: 0.933 :'county of body made of <Y>' <board>\n",
      "================================================================================\n",
      "Validation loss: -0.4976\n",
      "(0:57:8) step 2010/18666, epoch 0 Training Loss = 0.59382 :: 240.956 phrases/sec\n",
      "(0:57:26) step 2020/18666, epoch 0 Training Loss = 0.57993 :: 240.809 phrases/sec\n",
      "(0:57:42) step 2030/18666, epoch 0 Training Loss = 0.59138 :: 240.805 phrases/sec\n",
      "(0:57:57) step 2040/18666, epoch 0 Training Loss = 0.60869 :: 240.810 phrases/sec\n",
      "(0:58:12) step 2050/18666, epoch 0 Training Loss = 0.60079 :: 240.807 phrases/sec\n",
      "(0:58:26) step 2060/18666, epoch 0 Training Loss = 0.58812 :: 240.818 phrases/sec\n",
      "(0:58:40) step 2070/18666, epoch 0 Training Loss = 0.58882 :: 240.813 phrases/sec\n",
      "(0:58:54) step 2080/18666, epoch 0 Training Loss = 0.57386 :: 240.814 phrases/sec\n",
      "(0:59:7) step 2090/18666, epoch 0 Training Loss = 0.59819 :: 240.851 phrases/sec\n",
      "(0:59:22) step 2100/18666, epoch 0 Training Loss = 0.59189 :: 240.811 phrases/sec\n",
      "(0:59:35) step 2110/18666, epoch 0 Training Loss = 0.59180 :: 240.868 phrases/sec\n",
      "(0:59:49) step 2120/18666, epoch 0 Training Loss = 0.59362 :: 240.880 phrases/sec\n",
      "(1:0:3) step 2130/18666, epoch 0 Training Loss = 0.58303 :: 240.853 phrases/sec\n",
      "(1:0:16) step 2140/18666, epoch 0 Training Loss = 0.58655 :: 240.836 phrases/sec\n",
      "(1:0:31) step 2150/18666, epoch 0 Training Loss = 0.58689 :: 240.787 phrases/sec\n",
      "(1:0:45) step 2160/18666, epoch 0 Training Loss = 0.58202 :: 240.737 phrases/sec\n",
      "Saving model...\n",
      "Saving model to file: checkpoints/wikigru_50k.ckpt-2162-0\n",
      "(1:1:28) step 2170/18666, epoch 0 Training Loss = 0.59270 :: 240.719 phrases/sec\n",
      "(1:1:41) step 2180/18666, epoch 0 Training Loss = 0.59001 :: 240.706 phrases/sec\n",
      "(1:1:55) step 2190/18666, epoch 0 Training Loss = 0.58390 :: 240.674 phrases/sec\n",
      "(1:2:9) step 2200/18666, epoch 0 Training Loss = 0.59132 :: 240.663 phrases/sec\n",
      "(1:2:22) step 2210/18666, epoch 0 Training Loss = 0.57900 :: 240.668 phrases/sec\n",
      "(1:2:35) step 2220/18666, epoch 0 Training Loss = 0.59343 :: 240.700 phrases/sec\n",
      "(1:2:49) step 2230/18666, epoch 0 Training Loss = 0.59534 :: 240.695 phrases/sec\n",
      "(1:3:2) step 2240/18666, epoch 0 Training Loss = 0.58670 :: 240.726 phrases/sec\n",
      "(1:3:15) step 2250/18666, epoch 0 Training Loss = 0.59893 :: 240.754 phrases/sec\n",
      "(1:3:28) step 2260/18666, epoch 0 Training Loss = 0.59043 :: 240.771 phrases/sec\n",
      "(1:3:41) step 2270/18666, epoch 0 Training Loss = 0.59032 :: 240.796 phrases/sec\n",
      "(1:3:54) step 2280/18666, epoch 0 Training Loss = 0.59392 :: 240.831 phrases/sec\n",
      "(1:4:9) step 2290/18666, epoch 0 Training Loss = 0.59566 :: 240.702 phrases/sec\n",
      "(1:4:23) step 2300/18666, epoch 0 Training Loss = 0.60528 :: 240.708 phrases/sec\n",
      "(1:4:36) step 2310/18666, epoch 0 Training Loss = 0.59755 :: 240.724 phrases/sec\n",
      "(1:4:51) step 2320/18666, epoch 0 Training Loss = 0.59312 :: 240.627 phrases/sec\n",
      "(1:5:5) step 2330/18666, epoch 0 Training Loss = 0.57326 :: 240.602 phrases/sec\n",
      "(1:5:19) step 2340/18666, epoch 0 Training Loss = 0.57855 :: 240.550 phrases/sec\n",
      "(1:5:34) step 2350/18666, epoch 0 Training Loss = 0.58503 :: 240.469 phrases/sec\n",
      "(1:5:47) step 2360/18666, epoch 0 Training Loss = 0.57413 :: 240.462 phrases/sec\n",
      "(1:6:1) step 2370/18666, epoch 0 Training Loss = 0.58951 :: 240.490 phrases/sec\n",
      "(1:6:15) step 2380/18666, epoch 0 Training Loss = 0.59434 :: 240.438 phrases/sec\n",
      "(1:6:29) step 2390/18666, epoch 0 Training Loss = 0.58029 :: 240.370 phrases/sec\n",
      "(1:6:44) step 2400/18666, epoch 0 Training Loss = 0.57353 :: 240.274 phrases/sec\n",
      "(1:6:58) step 2410/18666, epoch 0 Training Loss = 0.60554 :: 240.209 phrases/sec\n",
      "(1:7:14) step 2420/18666, epoch 0 Training Loss = 0.58557 :: 240.081 phrases/sec\n",
      "(1:7:27) step 2430/18666, epoch 0 Training Loss = 0.59470 :: 240.058 phrases/sec\n",
      "(1:7:41) step 2440/18666, epoch 0 Training Loss = 0.57853 :: 240.069 phrases/sec\n",
      "(1:7:55) step 2450/18666, epoch 0 Training Loss = 0.57919 :: 240.023 phrases/sec\n",
      "(1:8:10) step 2460/18666, epoch 0 Training Loss = 0.58755 :: 239.936 phrases/sec\n",
      "(1:8:25) step 2470/18666, epoch 0 Training Loss = 0.58862 :: 239.829 phrases/sec\n",
      "(1:8:39) step 2480/18666, epoch 0 Training Loss = 0.57641 :: 239.787 phrases/sec\n",
      "(1:8:53) step 2490/18666, epoch 0 Training Loss = 0.57884 :: 239.742 phrases/sec\n",
      "(1:9:7) step 2500/18666, epoch 0 Training Loss = 0.59390 :: 239.695 phrases/sec\n",
      "================================================================================\n",
      "Top 25/4714 closest phrases to 'events of <OOV> <Y>' <toad>\n",
      "0: 1.000 :'events of <OOV> <Y>' <toad>\n",
      "1: 0.996 :'events of <X>' <series>\n",
      "2: 0.995 :'events including storm of <Y>' <century>\n",
      "3: 0.994 :'events hosted for <X>' <years>\n",
      "4: 0.994 :'events for appropriate speaking to <X>' <topics>\n",
      "5: 0.993 :'events in <Y>' <sector>\n",
      "6: 0.984 :'events taught <Y>' <shooters>\n",
      "7: 0.949 :'state of secretary was <X>' <he>\n",
      "8: 0.948 :'he spent part of <Y>' <summers>\n",
      "9: 0.948 :'he made played as <Y>' <catcher>\n",
      "10: 0.948 :'he was voice of <Y>' <teams>\n",
      "11: 0.948 :'he was father of <Y>' <actor>\n",
      "12: 0.947 :'he carried work in <Y>' <india>\n",
      "13: 0.947 :'state of secretary as served since <Y>' <january>\n",
      "14: 0.947 :'he was active in <Y>' <politics>\n",
      "15: 0.947 :'he tried in <Y>' <country>\n",
      "16: 0.947 :'system developed as result of <Y>' <experience>\n",
      "17: 0.947 :'he began working for <Y>' <cards>\n",
      "18: 0.947 :'he spent from returning to <Y>' <sydney>\n",
      "19: 0.946 :'he finds topics for <Y>' <books>\n",
      "20: 0.946 :'state of <Y>' <parks>\n",
      "21: 0.946 :'he attended university of <Y>' <virginia>\n",
      "22: 0.946 :'he held position for <Y>' <raiders>\n",
      "23: 0.946 :'he resolved <OOV> in <Y>' <army>\n",
      "24: 0.946 :'he returned to <Y>' <london>\n",
      "================================================================================\n",
      "Validation loss: -0.4976\n",
      "(1:11:6) step 2510/18666, epoch 0 Training Loss = 0.58529 :: 239.575 phrases/sec\n",
      "(1:11:21) step 2520/18666, epoch 0 Training Loss = 0.59349 :: 239.462 phrases/sec\n",
      "(1:11:35) step 2530/18666, epoch 0 Training Loss = 0.59201 :: 239.437 phrases/sec\n",
      "(1:11:51) step 2540/18666, epoch 0 Training Loss = 0.58474 :: 239.332 phrases/sec\n",
      "(1:12:7) step 2550/18666, epoch 0 Training Loss = 0.60236 :: 239.205 phrases/sec\n",
      "(1:12:24) step 2560/18666, epoch 0 Training Loss = 0.59275 :: 239.166 phrases/sec\n",
      "(1:12:41) step 2570/18666, epoch 0 Training Loss = 0.57719 :: 239.123 phrases/sec\n",
      "(1:12:57) step 2580/18666, epoch 0 Training Loss = 0.59840 :: 239.084 phrases/sec\n",
      "(1:13:12) step 2590/18666, epoch 0 Training Loss = 0.58867 :: 239.041 phrases/sec\n",
      "(1:13:26) step 2600/18666, epoch 0 Training Loss = 0.58855 :: 238.994 phrases/sec\n",
      "(1:13:41) step 2610/18666, epoch 0 Training Loss = 0.58061 :: 238.923 phrases/sec\n",
      "(1:13:57) step 2620/18666, epoch 0 Training Loss = 0.58795 :: 238.841 phrases/sec\n",
      "(1:14:12) step 2630/18666, epoch 0 Training Loss = 0.59544 :: 238.813 phrases/sec\n",
      "(1:14:26) step 2640/18666, epoch 0 Training Loss = 0.59245 :: 238.810 phrases/sec\n",
      "(1:14:48) step 2650/18666, epoch 0 Training Loss = 0.58023 :: 238.663 phrases/sec\n",
      "(1:15:2) step 2660/18666, epoch 0 Training Loss = 0.60046 :: 238.618 phrases/sec\n",
      "(1:15:16) step 2670/18666, epoch 0 Training Loss = 0.59598 :: 238.615 phrases/sec\n",
      "(1:15:30) step 2680/18666, epoch 0 Training Loss = 0.59094 :: 238.570 phrases/sec\n",
      "(1:15:43) step 2690/18666, epoch 0 Training Loss = 0.58760 :: 238.601 phrases/sec\n",
      "(1:15:58) step 2700/18666, epoch 0 Training Loss = 0.59294 :: 238.532 phrases/sec\n",
      "(1:16:12) step 2710/18666, epoch 0 Training Loss = 0.58861 :: 238.523 phrases/sec\n",
      "(1:16:26) step 2720/18666, epoch 0 Training Loss = 0.58400 :: 238.526 phrases/sec\n",
      "(1:16:40) step 2730/18666, epoch 0 Training Loss = 0.60185 :: 238.491 phrases/sec\n",
      "(1:16:53) step 2740/18666, epoch 0 Training Loss = 0.60045 :: 238.501 phrases/sec\n",
      "(1:17:7) step 2750/18666, epoch 0 Training Loss = 0.60373 :: 238.527 phrases/sec\n",
      "(1:17:21) step 2760/18666, epoch 0 Training Loss = 0.59319 :: 238.464 phrases/sec\n",
      "(1:17:36) step 2770/18666, epoch 0 Training Loss = 0.59129 :: 238.420 phrases/sec\n",
      "(1:17:49) step 2780/18666, epoch 0 Training Loss = 0.58718 :: 238.403 phrases/sec\n",
      "(1:18:3) step 2790/18666, epoch 0 Training Loss = 0.59315 :: 238.398 phrases/sec\n",
      "(1:18:17) step 2800/18666, epoch 0 Training Loss = 0.58945 :: 238.411 phrases/sec\n",
      "(1:18:31) step 2810/18666, epoch 0 Training Loss = 0.58990 :: 238.392 phrases/sec\n",
      "(1:18:45) step 2820/18666, epoch 0 Training Loss = 0.58511 :: 238.350 phrases/sec\n",
      "(1:18:59) step 2830/18666, epoch 0 Training Loss = 0.60275 :: 238.341 phrases/sec\n",
      "(1:19:14) step 2840/18666, epoch 0 Training Loss = 0.58327 :: 238.245 phrases/sec\n",
      "(1:19:29) step 2850/18666, epoch 0 Training Loss = 0.59136 :: 238.207 phrases/sec\n",
      "(1:19:44) step 2860/18666, epoch 0 Training Loss = 0.59671 :: 238.217 phrases/sec\n",
      "(1:19:57) step 2870/18666, epoch 0 Training Loss = 0.60287 :: 238.254 phrases/sec\n",
      "(1:20:11) step 2880/18666, epoch 0 Training Loss = 0.58766 :: 238.271 phrases/sec\n",
      "(1:20:24) step 2890/18666, epoch 0 Training Loss = 0.58803 :: 238.312 phrases/sec\n",
      "(1:20:37) step 2900/18666, epoch 0 Training Loss = 0.59992 :: 238.335 phrases/sec\n",
      "(1:20:50) step 2910/18666, epoch 0 Training Loss = 0.57729 :: 238.374 phrases/sec\n",
      "(1:21:4) step 2920/18666, epoch 0 Training Loss = 0.58811 :: 238.389 phrases/sec\n",
      "(1:21:17) step 2930/18666, epoch 0 Training Loss = 0.58802 :: 238.429 phrases/sec\n",
      "(1:21:30) step 2940/18666, epoch 0 Training Loss = 0.59607 :: 238.454 phrases/sec\n",
      "(1:21:43) step 2950/18666, epoch 0 Training Loss = 0.58490 :: 238.493 phrases/sec\n",
      "(1:21:56) step 2960/18666, epoch 0 Training Loss = 0.59622 :: 238.518 phrases/sec\n",
      "(1:22:9) step 2970/18666, epoch 0 Training Loss = 0.59351 :: 238.536 phrases/sec\n",
      "(1:22:22) step 2980/18666, epoch 0 Training Loss = 0.59197 :: 238.573 phrases/sec\n",
      "(1:22:35) step 2990/18666, epoch 0 Training Loss = 0.58768 :: 238.603 phrases/sec\n",
      "(1:22:48) step 3000/18666, epoch 0 Training Loss = 0.59121 :: 238.639 phrases/sec\n",
      "================================================================================\n",
      "Top 25/4714 closest phrases to 'bases with <Y>' <groups>\n",
      "0: 1.000 :'bases with <Y>' <groups>\n",
      "1: 0.996 :'bases steal in <Y>' <season>\n",
      "2: 0.915 :'absence during acted as <Y>' <secretary>\n",
      "3: 0.908 :'tanks in <Y>' <britain>\n",
      "4: 0.907 :'stevens died on <Y>' <august>\n",
      "5: 0.905 :'ground featured included <Y>' <interview>\n",
      "6: 0.903 :'absence after making return to <X>' <group>\n",
      "7: 0.903 :'death had <Y>' <effect>\n",
      "8: 0.903 :'act ordered <Y>' <districts>\n",
      "9: 0.903 :'problems with <Y>' <quality>\n",
      "10: 0.902 :'problems shared <Y>' <goals>\n",
      "11: 0.901 :'pair of <Y>' <transmit>\n",
      "12: 0.898 :'communication <Y>' <education>\n",
      "13: 0.898 :'problems caused <Y>' <periods>\n",
      "14: 0.898 :'blow suffer joined <Y>' <robinson>\n",
      "15: 0.897 :'act of <X>' <passage>\n",
      "16: 0.897 :'tanks of total destroyed by <Y>' <infantry>\n",
      "17: 0.897 :'sediments <Y>' <sediments>\n",
      "18: 0.897 :'act created levels of <Y>' <courts>\n",
      "19: 0.897 :'problems be <Y>' <anything>\n",
      "20: 0.896 :'death after came used <Y>' <patronage>\n",
      "21: 0.896 :'death suffered <X>' <times>\n",
      "22: 0.896 :'countries including <Y>' <uk>\n",
      "23: 0.896 :'cigarettes cause contain headaches <Y>' <nausea>\n",
      "24: 0.896 :'bag put on <Y>' <bed>\n",
      "================================================================================\n",
      "Validation loss: -0.4976\n",
      "(1:24:57) step 3010/18666, epoch 0 Training Loss = 0.58123 :: 238.661 phrases/sec\n",
      "(1:25:12) step 3020/18666, epoch 0 Training Loss = 0.59039 :: 238.689 phrases/sec\n",
      "(1:25:26) step 3030/18666, epoch 0 Training Loss = 0.58796 :: 238.715 phrases/sec\n",
      "(1:25:39) step 3040/18666, epoch 0 Training Loss = 0.59129 :: 238.742 phrases/sec\n",
      "(1:25:52) step 3050/18666, epoch 0 Training Loss = 0.60064 :: 238.775 phrases/sec\n",
      "(1:26:6) step 3060/18666, epoch 0 Training Loss = 0.59939 :: 238.798 phrases/sec\n",
      "(1:26:19) step 3070/18666, epoch 0 Training Loss = 0.59112 :: 238.837 phrases/sec\n",
      "(1:26:32) step 3080/18666, epoch 0 Training Loss = 0.59475 :: 238.872 phrases/sec\n",
      "(1:26:45) step 3090/18666, epoch 0 Training Loss = 0.60779 :: 238.909 phrases/sec\n",
      "(1:26:59) step 3100/18666, epoch 0 Training Loss = 0.59525 :: 238.939 phrases/sec\n",
      "(1:27:12) step 3110/18666, epoch 0 Training Loss = 0.59897 :: 238.967 phrases/sec\n",
      "(1:27:25) step 3120/18666, epoch 0 Training Loss = 0.59588 :: 239.004 phrases/sec\n",
      "(1:27:38) step 3130/18666, epoch 0 Training Loss = 0.58947 :: 239.037 phrases/sec\n",
      "(1:27:51) step 3140/18666, epoch 0 Training Loss = 0.59844 :: 239.070 phrases/sec\n",
      "(1:28:5) step 3150/18666, epoch 0 Training Loss = 0.58306 :: 239.088 phrases/sec\n",
      "(1:28:18) step 3160/18666, epoch 0 Training Loss = 0.59128 :: 239.110 phrases/sec\n",
      "(1:28:32) step 3170/18666, epoch 0 Training Loss = 0.57792 :: 239.097 phrases/sec\n",
      "(1:28:45) step 3180/18666, epoch 0 Training Loss = 0.59037 :: 239.096 phrases/sec\n",
      "(1:28:58) step 3190/18666, epoch 0 Training Loss = 0.59323 :: 239.128 phrases/sec\n",
      "(1:29:11) step 3200/18666, epoch 0 Training Loss = 0.59006 :: 239.162 phrases/sec\n",
      "(1:29:24) step 3210/18666, epoch 0 Training Loss = 0.59392 :: 239.198 phrases/sec\n",
      "(1:29:37) step 3220/18666, epoch 0 Training Loss = 0.60873 :: 239.229 phrases/sec\n",
      "(1:29:51) step 3230/18666, epoch 0 Training Loss = 0.59230 :: 239.261 phrases/sec\n",
      "(1:30:4) step 3240/18666, epoch 0 Training Loss = 0.58464 :: 239.291 phrases/sec\n",
      "(1:30:17) step 3250/18666, epoch 0 Training Loss = 0.58447 :: 239.308 phrases/sec\n",
      "(1:30:30) step 3260/18666, epoch 0 Training Loss = 0.59018 :: 239.330 phrases/sec\n",
      "(1:30:43) step 3270/18666, epoch 0 Training Loss = 0.59245 :: 239.357 phrases/sec\n",
      "(1:30:56) step 3280/18666, epoch 0 Training Loss = 0.60020 :: 239.385 phrases/sec\n",
      "(1:31:10) step 3290/18666, epoch 0 Training Loss = 0.59150 :: 239.406 phrases/sec\n",
      "Saving model...\n",
      "Saving model to file: checkpoints/wikigru_50k.ckpt-3296-0\n",
      "(1:31:39) step 3300/18666, epoch 0 Training Loss = 0.59639 :: 239.431 phrases/sec\n",
      "(1:31:52) step 3310/18666, epoch 0 Training Loss = 0.57614 :: 239.460 phrases/sec\n",
      "(1:32:5) step 3320/18666, epoch 0 Training Loss = 0.58356 :: 239.490 phrases/sec\n",
      "(1:32:18) step 3330/18666, epoch 0 Training Loss = 0.59119 :: 239.508 phrases/sec\n",
      "(1:32:31) step 3340/18666, epoch 0 Training Loss = 0.58579 :: 239.534 phrases/sec\n",
      "(1:32:44) step 3350/18666, epoch 0 Training Loss = 0.59039 :: 239.567 phrases/sec\n",
      "(1:32:57) step 3360/18666, epoch 0 Training Loss = 0.59369 :: 239.602 phrases/sec\n",
      "(1:33:10) step 3370/18666, epoch 0 Training Loss = 0.59236 :: 239.620 phrases/sec\n",
      "(1:33:23) step 3380/18666, epoch 0 Training Loss = 0.59062 :: 239.644 phrases/sec\n",
      "(1:33:36) step 3390/18666, epoch 0 Training Loss = 0.58779 :: 239.674 phrases/sec\n",
      "(1:33:49) step 3400/18666, epoch 0 Training Loss = 0.60245 :: 239.707 phrases/sec\n",
      "(1:34:2) step 3410/18666, epoch 0 Training Loss = 0.58990 :: 239.728 phrases/sec\n",
      "(1:34:15) step 3420/18666, epoch 0 Training Loss = 0.59188 :: 239.752 phrases/sec\n",
      "(1:34:28) step 3430/18666, epoch 0 Training Loss = 0.59391 :: 239.782 phrases/sec\n",
      "(1:34:41) step 3440/18666, epoch 0 Training Loss = 0.58147 :: 239.800 phrases/sec\n",
      "(1:34:54) step 3450/18666, epoch 0 Training Loss = 0.57326 :: 239.825 phrases/sec\n",
      "(1:35:7) step 3460/18666, epoch 0 Training Loss = 0.58613 :: 239.860 phrases/sec\n",
      "(1:35:20) step 3470/18666, epoch 0 Training Loss = 0.59027 :: 239.876 phrases/sec\n",
      "(1:35:33) step 3480/18666, epoch 0 Training Loss = 0.58357 :: 239.911 phrases/sec\n",
      "(1:35:46) step 3490/18666, epoch 0 Training Loss = 0.58224 :: 239.934 phrases/sec\n",
      "(1:35:59) step 3500/18666, epoch 0 Training Loss = 0.59473 :: 239.960 phrases/sec\n",
      "================================================================================\n",
      "Top 25/4714 closest phrases to 'airfield was <X>' <it>\n",
      "0: 1.000 :'airfield was <X>' <it>\n",
      "1: 0.890 :'reason is used <Y>' <strategies>\n",
      "2: 0.888 :'reason for obscured <X>' <light>\n",
      "3: 0.887 :'line below <X>' <%>\n",
      "4: 0.887 :'line below <X>' <%>\n",
      "5: 0.886 :'increase affected <Y>' <waterways>\n",
      "6: 0.885 :'insurance provide <OOV> <X>' <employers>\n",
      "7: 0.883 :'line behind <NUM> including <X>' <tackles>\n",
      "8: 0.883 :'conference stood <X>' <chamber>\n",
      "9: 0.882 :'line below were of <X>' <families>\n",
      "10: 0.882 :'line form ordered <X>' <army>\n",
      "11: 0.882 :'line served <Y>' <colliery>\n",
      "12: 0.881 :'conference of champion was <X>' <oklahoma>\n",
      "13: 0.881 :'line including cotton <Y>' <coats>\n",
      "14: 0.880 :'line cans has <X>' <dock>\n",
      "15: 0.879 :'al pleaded return to <Y>' <u.s.>\n",
      "16: 0.878 :'they represent work <X>' <they>\n",
      "17: 0.877 :'relationship among tourism consumption <Y>' <direction>\n",
      "18: 0.877 :'they arrive is <X>' <cohort>\n",
      "19: 0.876 :'category chosen by <Y>' <public>\n",
      "20: 0.876 :'group markets <Y>' <plants>\n",
      "21: 0.876 :'they drove <Y>' <herds>\n",
      "22: 0.876 :'nymphs of <X>' <names>\n",
      "23: 0.876 :'death suffered <X>' <times>\n",
      "24: 0.876 :'road connects across <Y>' <river>\n",
      "================================================================================\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-c8dffd4bb8b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0mrun_validation_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_nearby\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# check out the nearby phrases in the validation set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mvalid_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDH\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m             \u001b[0mvalid_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mvalid_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Validation loss: %0.4f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mvalid_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/thomaseffland/Development/Relation-Extraction/relembed_single.py\u001b[0m in \u001b[0;36mvalidation_loss\u001b[0;34m(self, valid_phrases, valid_targets, valid_labels, valid_lengths, valid_predict_x)\u001b[0m\n\u001b[1;32m    740\u001b[0m                                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mvalid_targets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m                                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_labels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mvalid_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 742\u001b[0;31m                                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_lengths\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mvalid_lengths\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    743\u001b[0m                                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_predict_x\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mvalid_predict_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    744\u001b[0m                                             self._keep_prob:1.0})\n",
      "\u001b[0;32m/Users/thomaseffland/.virtualenvs/rel/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict)\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;34m`\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m`\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mdoesn\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0mt\u001b[0m \u001b[0mexist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m     \"\"\"\n\u001b[0;32m--> 315\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mpartial_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/thomaseffland/.virtualenvs/rel/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict)\u001b[0m\n\u001b[1;32m    509\u001b[0m     \u001b[0;31m# Run request and get response.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m     results = self._do_run(handle, target_list, unique_fetches,\n\u001b[0;32m--> 511\u001b[0;31m                            feed_dict_string)\n\u001b[0m\u001b[1;32m    512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m     \u001b[0;31m# User may have fetched the same tensor multiple times, but we\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/thomaseffland/.virtualenvs/rel/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict)\u001b[0m\n\u001b[1;32m    562\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m--> 564\u001b[0;31m                            target_list)\n\u001b[0m\u001b[1;32m    565\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/thomaseffland/.virtualenvs/rel/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m    569\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    572\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStatusNotOK\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m       \u001b[0me_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me_traceback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/thomaseffland/.virtualenvs/rel/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list)\u001b[0m\n\u001b[1;32m    553\u001b[0m       \u001b[0;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# reload(nn)\n",
    "# drnn = reset_drnn(model_name='wikicluster_state', bi=False, word_embed_size=None, num_clusters=num_clusters)\n",
    "# reload(dh)\n",
    "# DH = dh.DataHandler('data/semeval_wiki_sdp_include_single_10000', valid_percent=10, shuffle_seed=shuffle_seed) # for semeval\n",
    "reload(nn)\n",
    "drnn = reset_drnn(model_name='wikigru_50k', bi=True, word_embed_size=None)\n",
    "# hyperparameters\n",
    "num_epochs = 3\n",
    "batch_size =25\n",
    "target_neg=True\n",
    "neg_per = 10\n",
    "neg_level = 1\n",
    "num_nearby = 25\n",
    "nearby_mod = 500\n",
    "sample_power = .75\n",
    "DH.scale_vocab_dist(sample_power)\n",
    "DH.scale_target_dist(sample_power)\n",
    "\n",
    "# bookkeeping\n",
    "num_steps = DH.num_steps(batch_size)\n",
    "total_step = 1\n",
    "save_interval = 30 * 60 # half hour in seconds\n",
    "save_time = time()\n",
    "\n",
    "#timing stuff\n",
    "start = time()\n",
    "fit_time = 0\n",
    "nearby_time = 0\n",
    "\n",
    "best_valid = 100000\n",
    "best_model = None\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    DH.shuffle_data()\n",
    "    for step , batch in enumerate(DH.batches(batch_size, target_neg=target_neg, \n",
    "                                             neg_per=neg_per, neg_level=neg_level)):\n",
    "#         print(batch[-1])\n",
    "        t0 = time()\n",
    "        loss = drnn.partial_unsup_fit(*batch)\n",
    "        fit_time = (fit_time * float(total_step) +  time() - t0) / (total_step + 1) # running average\n",
    "        if step % 10 == 0:\n",
    "            m,s = divmod(time()-start, 60)\n",
    "            h,m = divmod(m, 60)\n",
    "#             left = time_left(num_epochs, num_steps, fit_time, nearby_time, start, nearby_mod)\n",
    "#             ml,sl = divmod(left, 60)\n",
    "#             hl,ml = divmod(ml, 60)\n",
    "            pps = batch_size*(neg_per + 1) / fit_time \n",
    "            print(\"(%i:%i:%i) step %i/%i, epoch %i Training Loss = %1.5f :: %0.3f phrases/sec\" \n",
    "                  % (h,m,s, step, num_steps, epoch, loss, pps))\n",
    "        if (total_step-1) % nearby_mod == 0: # do one right away so we get a good timing estimate\n",
    "            t0 = time()\n",
    "            run_validation_test(num_nearby) # check out the nearby phrases in the validation set\n",
    "            valid_batch = DH.validation_batch()\n",
    "            valid_loss = drnn.validation_loss(*valid_batch)\n",
    "            \n",
    "            print(\"Validation loss: %0.4f\" % valid_loss)\n",
    "            nearby_time = (nearby_time * float(total_step) + time() - t0) / (total_step + 1) # running average\n",
    "#             if valid_loss <= best_valid:\n",
    "#                 best_valid = valid_loss\n",
    "#                 best_model = drnn.checkpoint()\n",
    "        if (time() - save_time) > save_interval:\n",
    "            print(\"Saving model...\")\n",
    "            drnn.checkpoint()\n",
    "            save_time = time()\n",
    "        total_step +=1\n",
    "drnn.checkpoint()\n",
    "print(\"Best model was %s\" % best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def confusion_matrix(preds, labels, label_set):\n",
    "    size = len(label_set)\n",
    "    matrix = np.zeros([size, size]) # rows are predictions, columns are truths\n",
    "    # fill in matrix\n",
    "    for p, l in zip(preds, labels):\n",
    "        matrix[p,l] += 1\n",
    "    # compute class specific scores\n",
    "    class_precision = np.zeros(size)\n",
    "    class_recall = np.zeros(size)\n",
    "    for label in range(size):\n",
    "        tp = matrix[label, label]\n",
    "        fp = np.sum(matrix[label, :]) - tp\n",
    "        fn = np.sum(matrix[:, label]) - tp\n",
    "        class_precision[label] = tp/float(tp + fp) if tp or fp else 0\n",
    "        class_recall[label] = tp/float(tp + fn) if tp or fn else 0\n",
    "    micro_f1 = np.array([2*(p*r)/(p+r) if p or r else 0 for (p, r) in zip(class_precision, class_recall)])\n",
    "    avg_precision = np.mean(class_precision)\n",
    "    avg_recall = np.mean(class_recall)\n",
    "    macro_f1 = (2*avg_precision*avg_recall) / (avg_precision + avg_recall) if avg_precision and avg_recall else 0\n",
    "    stats = {'micro_precision':class_precision*100,\n",
    "             'micro_recall':class_recall*100, \n",
    "             'micro_f1':micro_f1*100,\n",
    "             'macro_precision':avg_precision*100, \n",
    "             'macro_recall':avg_recall*100,\n",
    "             'macro_f1':macro_f1*100}\n",
    "    return matrix, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "zip_train = zip(train['raws'], train['sents'], train['sdps'], train['targets'], train['labels'])\n",
    "zip_valid = zip(valid['raws'], valid['sents'], valid['sdps'], valid['targets'], valid['labels'])\n",
    "zip_test = zip(test['raws'], test['sents'], test['sdps'], test['targets'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "Unsuccessful TensorSliceReader constructor: Failed to find any matching files for checkpoints/wikiall_50k.ckpt-44978-0\n\t [[Node: save/restore_slice = RestoreSlice[dt=DT_FLOAT, preferred_shard=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save/Const_0, save/restore_slice/tensor_name, save/restore_slice/shape_and_slice)]]\nCaused by op u'save/restore_slice', defined at:\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/runpy.py\", line 162, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/Users/thomaseffland/.virtualenvs/rel/lib/python2.7/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/Users/thomaseffland/.virtualenvs/rel/lib/python2.7/site-packages/traitlets/config/application.py\", line 589, in launch_instance\n    app.start()\n  File \"/Users/thomaseffland/.virtualenvs/rel/lib/python2.7/site-packages/ipykernel/kernelapp.py\", line 440, in start\n    ioloop.IOLoop.instance().start()\n  File \"/Users/thomaseffland/.virtualenvs/rel/lib/python2.7/site-packages/zmq/eventloop/ioloop.py\", line 162, in start\n    super(ZMQIOLoop, self).start()\n  File \"/Users/thomaseffland/.virtualenvs/rel/lib/python2.7/site-packages/tornado/ioloop.py\", line 883, in start\n    handler_func(fd_obj, events)\n  File \"/Users/thomaseffland/.virtualenvs/rel/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/thomaseffland/.virtualenvs/rel/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/Users/thomaseffland/.virtualenvs/rel/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/thomaseffland/.virtualenvs/rel/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/thomaseffland/.virtualenvs/rel/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/thomaseffland/.virtualenvs/rel/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/thomaseffland/.virtualenvs/rel/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/thomaseffland/.virtualenvs/rel/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 391, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/thomaseffland/.virtualenvs/rel/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 199, in do_execute\n    shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/thomaseffland/.virtualenvs/rel/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2723, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/thomaseffland/.virtualenvs/rel/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2825, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/thomaseffland/.virtualenvs/rel/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2885, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-18-5a0d98df9749>\", line 6, in <module>\n    drnn = reset_drnn(model_name='wikiall_50k', bi=True, word_embed_size=None)\n  File \"<ipython-input-12-b676f862ec04>\", line 56, in reset_drnn\n    drnn = nn.RelEmbed(config)\n  File \"relembed_single.py\", line 100, in __init__\n    self.saver = tf.train.Saver(tf.all_variables(), max_to_keep=config['max_to_keep'])\n  File \"/Users/thomaseffland/.virtualenvs/rel/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 784, in __init__\n    restore_sequentially=restore_sequentially)\n  File \"/Users/thomaseffland/.virtualenvs/rel/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 454, in build\n    filename_tensor, vars_to_save, restore_sequentially, reshape)\n  File \"/Users/thomaseffland/.virtualenvs/rel/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 212, in _AddRestoreOps\n    values = self.restore_op(filename_tensor, vs, preferred_shard)\n  File \"/Users/thomaseffland/.virtualenvs/rel/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 127, in restore_op\n    preferred_shard=preferred_shard)\n  File \"/Users/thomaseffland/.virtualenvs/rel/lib/python2.7/site-packages/tensorflow/python/ops/io_ops.py\", line 193, in _restore_slice\n    preferred_shard, name=name)\n  File \"/Users/thomaseffland/.virtualenvs/rel/lib/python2.7/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 271, in _restore_slice\n    preferred_shard=preferred_shard, name=name)\n  File \"/Users/thomaseffland/.virtualenvs/rel/lib/python2.7/site-packages/tensorflow/python/ops/op_def_library.py\", line 655, in apply_op\n    op_def=op_def)\n  File \"/Users/thomaseffland/.virtualenvs/rel/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2040, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/Users/thomaseffland/.virtualenvs/rel/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1087, in __init__\n    self._traceback = _extract_stack()\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-2df9aba24156>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# reload(nn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# drnn = reset_drnn(model_name='wikisingle', bi=True, hidden_size=300, word_embed_size=None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'checkpoints/wikiall_50k.ckpt-44978-0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# drnn.random_restart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/thomaseffland/Development/Relation-Extraction/relembed_single.pyc\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, model_ckpt_path)\u001b[0m\n\u001b[1;32m    826\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_ckpt_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_ckpt_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrestore_unsupervised\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_ckpt_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/thomaseffland/.virtualenvs/rel/lib/python2.7/site-packages/tensorflow/python/training/saver.pyc\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, sess, save_path)\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \"\"\"\n\u001b[1;32m   1007\u001b[0m     sess.run(self.saver_def.restore_op_name,\n\u001b[0;32m-> 1008\u001b[0;31m              {self.saver_def.filename_tensor_name: save_path})\n\u001b[0m\u001b[1;32m   1009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1010\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/thomaseffland/.virtualenvs/rel/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict)\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;34m`\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m`\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mdoesn\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0mt\u001b[0m \u001b[0mexist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m     \"\"\"\n\u001b[0;32m--> 315\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mpartial_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/thomaseffland/.virtualenvs/rel/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict)\u001b[0m\n\u001b[1;32m    509\u001b[0m     \u001b[0;31m# Run request and get response.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m     results = self._do_run(handle, target_list, unique_fetches,\n\u001b[0;32m--> 511\u001b[0;31m                            feed_dict_string)\n\u001b[0m\u001b[1;32m    512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m     \u001b[0;31m# User may have fetched the same tensor multiple times, but we\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/thomaseffland/.virtualenvs/rel/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict)\u001b[0m\n\u001b[1;32m    562\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m--> 564\u001b[0;31m                            target_list)\n\u001b[0m\u001b[1;32m    565\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/thomaseffland/.virtualenvs/rel/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m         raise errors._make_specific_exception(node_def, op, error_message,\n\u001b[0;32m--> 586\u001b[0;31m                                               e.code)\n\u001b[0m\u001b[1;32m    587\u001b[0m         \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m       \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me_traceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for checkpoints/wikiall_50k.ckpt-44978-0\n\t [[Node: save/restore_slice = RestoreSlice[dt=DT_FLOAT, preferred_shard=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save/Const_0, save/restore_slice/tensor_name, save/restore_slice/shape_and_slice)]]\nCaused by op u'save/restore_slice', defined at:\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/runpy.py\", line 162, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/Users/thomaseffland/.virtualenvs/rel/lib/python2.7/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/Users/thomaseffland/.virtualenvs/rel/lib/python2.7/site-packages/traitlets/config/application.py\", line 589, in launch_instance\n    app.start()\n  File \"/Users/thomaseffland/.virtualenvs/rel/lib/python2.7/site-packages/ipykernel/kernelapp.py\", line 440, in start\n    ioloop.IOLoop.instance().start()\n  File \"/Users/thomaseffland/.virtualenvs/rel/lib/python2.7/site-packages/zmq/eventloop/ioloop.py\", line 162, in start\n    super(ZMQIOLoop, self).start()\n  File \"/Users/thomaseffland/.virtualenvs/rel/lib/python2.7/site-packages/tornado/ioloop.py\", line 883, in start\n    handler_func(fd_obj, events)\n  File \"/Users/thomaseffland/.virtualenvs/rel/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/thomaseffland/.virtualenvs/rel/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/Users/thomaseffland/.virtualenvs/rel/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/thomaseffland/.virtualenvs/rel/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/thomaseffland/.virtualenvs/rel/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/thomaseffland/.virtualenvs/rel/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/thomaseffland/.virtualenvs/rel/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/thomaseffland/.virtualenvs/rel/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 391, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/thomaseffland/.virtualenvs/rel/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 199, in do_execute\n    shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/thomaseffland/.virtualenvs/rel/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2723, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/thomaseffland/.virtualenvs/rel/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2825, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/thomaseffland/.virtualenvs/rel/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2885, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-18-5a0d98df9749>\", line 6, in <module>\n    drnn = reset_drnn(model_name='wikiall_50k', bi=True, word_embed_size=None)\n  File \"<ipython-input-12-b676f862ec04>\", line 56, in reset_drnn\n    drnn = nn.RelEmbed(config)\n  File \"relembed_single.py\", line 100, in __init__\n    self.saver = tf.train.Saver(tf.all_variables(), max_to_keep=config['max_to_keep'])\n  File \"/Users/thomaseffland/.virtualenvs/rel/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 784, in __init__\n    restore_sequentially=restore_sequentially)\n  File \"/Users/thomaseffland/.virtualenvs/rel/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 454, in build\n    filename_tensor, vars_to_save, restore_sequentially, reshape)\n  File \"/Users/thomaseffland/.virtualenvs/rel/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 212, in _AddRestoreOps\n    values = self.restore_op(filename_tensor, vs, preferred_shard)\n  File \"/Users/thomaseffland/.virtualenvs/rel/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 127, in restore_op\n    preferred_shard=preferred_shard)\n  File \"/Users/thomaseffland/.virtualenvs/rel/lib/python2.7/site-packages/tensorflow/python/ops/io_ops.py\", line 193, in _restore_slice\n    preferred_shard, name=name)\n  File \"/Users/thomaseffland/.virtualenvs/rel/lib/python2.7/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 271, in _restore_slice\n    preferred_shard=preferred_shard, name=name)\n  File \"/Users/thomaseffland/.virtualenvs/rel/lib/python2.7/site-packages/tensorflow/python/ops/op_def_library.py\", line 655, in apply_op\n    op_def=op_def)\n  File \"/Users/thomaseffland/.virtualenvs/rel/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2040, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/Users/thomaseffland/.virtualenvs/rel/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1087, in __init__\n    self._traceback = _extract_stack()\n"
     ]
    }
   ],
   "source": [
    "# reload(nn)\n",
    "# drnn = reset_drnn(model_name='wikisingle', bi=True, hidden_size=300, word_embed_size=None)\n",
    "drnn.restore('wikiall_50k.ckpt-48099-1531')\n",
    "# drnn.random_restart\n",
    "batch_size = 50\n",
    "num_steps = len(train['labels']) // batch_size\n",
    "num_epochs = 50\n",
    "display_mod = 10\n",
    "valid_mod = 50\n",
    "best_valid = 10e6\n",
    "early_stop_model = None\n",
    "start = time()\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    random.shuffle(zip_train) # shuffling should only happen once per epoch\n",
    "    _, _, sdps, targets, labels = zip(*zip_train)\n",
    "    for step in range(num_steps): # num_steps\n",
    "        class_batch = DH.classification_batch(batch_size, sdps, targets, labels, \n",
    "                                              offset=step, shuffle=False, singles=True)\n",
    "        loss, xent = drnn.partial_class_fit(*class_batch)\n",
    "        if step % display_mod == 0:   \n",
    "            m,s = divmod(time()-start, 60)\n",
    "            h,m = divmod(m, 60)\n",
    "            print(\"(%i:%i:%i) s %i/%i, e %i avg class xent loss = %0.4f, total loss = %0.4f\" \n",
    "                  % (h,m,s, step, num_steps, epoch, xent, loss))\n",
    "        if step % valid_mod == 0:\n",
    "            valid_batch = DH.classification_batch(len(valid['labels']), valid['sdps'], valid['targets'], valid['labels'], singles=True)\n",
    "            valid_loss, valid_xent = drnn.validation_class_loss(*valid_batch)\n",
    "            m,s = divmod(time()-start, 60)\n",
    "            h,m = divmod(m, 60)\n",
    "            print(\"=\"*80)\n",
    "            print(\"(%i:%i:%i) s %i/%i, e %i validation avg class xent loss = %0.4f, total loss = %0.4f\" \n",
    "                  % (h,m,s, step, num_steps, epoch, valid_xent, valid_loss))\n",
    "            print(\"=\"*80)\n",
    "            if valid_xent < best_valid:\n",
    "                print(\"New best validation\")\n",
    "                best_valid = valid_xent\n",
    "                early_stop_model = drnn.checkpoint()\n",
    "    x_p, y_p, x_t, y_t, _, lens = DH.classification_batch(len(valid['labels']), valid['sdps'], valid['targets'], valid['labels'], singles=True)\n",
    "    label_set = set(train['labels'])\n",
    "    preds = drnn.predict(x_p, y_p, x_t, y_t, lens)\n",
    "    cm, stats = confusion_matrix(preds, valid['labels'], label_set)\n",
    "    print(\"Macro F1: %2.4f\" % stats['macro_f1'])\n",
    "# do a final validation\n",
    "valid_loss, valid_xent = drnn.validation_class_loss(*valid_batch)\n",
    "m,s = divmod(time()-start, 60)\n",
    "h,m = divmod(m, 60)\n",
    "print(\"=\"*80)\n",
    "print(\"(%i:%i:%i) s %i/%i, e %i validation avg class xent loss = %0.4f, total loss = %0.4f\" \n",
    "                  % (h,m,s, step, num_steps, epoch, valid_xent, valid_loss))\n",
    "print(\"=\"*80)\n",
    "\n",
    "\n",
    "model_file = drnn.checkpoint()\n",
    "if valid_xent < best_valid:\n",
    "    best_valid = valid_xent\n",
    "    early_stop_model = model_file\n",
    "\n",
    "# now take the best of all\n",
    "print(\"best model was %s\" % early_stop_model)\n",
    "# drnn.restore(early_stop_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "drnn.restore(early_stop_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# write out predictions for test set\n",
    "test_batch = DH.classification_batch(len(test['targets']), test['sdps'], test['targets'], \n",
    "                                     np.zeros(len(test['targets'])), shuffle=False, singles=True)\n",
    "preds = drnn.predict(test_batch[0], test_batch[1],\n",
    "                     test_batch[2], test_batch[3],\n",
    "                     test_batch[5])\n",
    "with open('SemEval2010_task8_all_data/test_pred.txt', 'w') as f:\n",
    "    i = 8001\n",
    "    for pred in preds:\n",
    "        f.write(\"%i\\t%s\\n\" % (i, int2label[pred]))\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "./SemEval2010_task8_all_data/SemEval2010_task8_scorer-v1.2/semeval2010_task8_scorer-v1.2.pl \\\n",
    "SemEval2010_task8_all_data/test_pred.txt SemEval2010_task8_all_data/test_keys.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embeds = drnn.score_w.eval()\n",
    "fig, (ax0, ax1) = plt.subplots(1,2, figsize=(12,9))\n",
    "im = ax0.imshow(embeds, aspect='auto', interpolation='nearest')\n",
    "\n",
    "embeds = drnn.score_bias.eval().reshape([1,-1])\n",
    "im = ax1.imshow(embeds, aspect='auto', interpolation='nearest')\n",
    "\n",
    "fig.subplots_adjust(right=0.8)\n",
    "cbar_ax = fig.add_axes([0.85, 0.15, 0.05, 0.7])\n",
    "fig.colorbar(im, cax=cbar_ax)\n",
    "\n",
    "# ax0.set_yticks([0,300,600,900])\n",
    "# ax0.set_xticks([])\n",
    "# ax0.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# embeddings\n",
    "embeds = word_embeddings\n",
    "fig, (ax0, ax1) = plt.subplots(1,2, figsize=(12,9))\n",
    "im = ax0.imshow(embeds, aspect='auto', interpolation='nearest', vmin=-2, vmax=2)\n",
    "\n",
    "embeds = drnn._word_embeddings.eval()\n",
    "ax1.imshow(embeds, aspect='auto', interpolation='nearest', vmin=-2, vmax=2)\n",
    "\n",
    "fig.subplots_adjust(right=0.8)\n",
    "cbar_ax = fig.add_axes([0.85, 0.15, 0.05, 0.7])\n",
    "fig.colorbar(im, cax=cbar_ax)\n",
    "\n",
    "### Top half is input, bottom is r*candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# dep and pos embeddings\n",
    "embeds = drnn._dependency_embeddings.eval()\n",
    "fig, (ax0, ax1) = plt.subplots(1,2, figsize=(12,9))\n",
    "im = ax0.imshow(embeds, aspect='auto', interpolation='nearest', vmin=-1, vmax=1)\n",
    "ax0.set_yticklabels(DH._dep_vocab)\n",
    "ax0.set_yticks(range(len(DH._dep_vocab)))\n",
    "\n",
    "embeds = drnn._pos_embeddings.eval()\n",
    "ax1.imshow(embeds, aspect='auto', interpolation='nearest', vmin=-1, vmax=1)\n",
    "ax1.set_yticklabels(DH._pos_vocab)\n",
    "ax1.set_yticks(range(len(DH._pos_vocab)))\n",
    "\n",
    "fig.subplots_adjust(right=0.8)\n",
    "cbar_ax = fig.add_axes([0.85, 0.15, 0.05, 0.7])\n",
    "fig.colorbar(im, cax=cbar_ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# GRU candidate matrix\n",
    "embeds = tf.get_variable(\"FW/RNN/GRUCell/Candidate/Matrix\").eval()\n",
    "fig, (ax0, ax1) = plt.subplots(1,2, figsize=(12,9))\n",
    "im = ax0.imshow(embeds, aspect='auto', interpolation='nearest')\n",
    "\n",
    "# embeds = drnn._cand_bias.eval().reshape([1,-1])\n",
    "# ax1.imshow(embeds, aspect='auto', interpolation='nearest')\n",
    "\n",
    "fig.subplots_adjust(right=0.8)\n",
    "cbar_ax = fig.add_axes([0.85, 0.15, 0.05, 0.7])\n",
    "fig.colorbar(im, cax=cbar_ax)\n",
    "ax0.set_yticks([0,300,325,350,650])\n",
    "ax0.set_xticks([0, 300])\n",
    "ax0.grid()\n",
    "### Top half is input, bottom is r*candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# GRU GATE MATRIX\n",
    "embeds = drnn._gate_matrix.eval()\n",
    "fig, (ax0, ax1) = plt.subplots(1,2, figsize=(12,9))\n",
    "im = ax0.imshow(embeds, aspect='auto', interpolation='nearest')\n",
    "\n",
    "embeds = drnn._gate_bias.eval().reshape([1,-1])\n",
    "im = ax1.imshow(embeds, aspect='auto', interpolation='nearest')\n",
    "\n",
    "fig.subplots_adjust(right=0.8)\n",
    "cbar_ax = fig.add_axes([0.85, 0.15, 0.05, 0.7])\n",
    "fig.colorbar(im, cax=cbar_ax)\n",
    "ax0.set_yticks([0,300,325,350,650])\n",
    "ax0.set_xticks([0, 300, 600])\n",
    "ax0.grid()\n",
    "# Left is r, right is z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
