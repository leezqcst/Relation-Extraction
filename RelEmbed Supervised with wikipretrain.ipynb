{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import collections\n",
    "import random\n",
    "from time import time\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA, FastICA\n",
    "\n",
    "import data_handler as dh\n",
    "import semeval_data_helper as sdh\n",
    "\n",
    "\n",
    "# plot settings\n",
    "% matplotlib inline\n",
    "# print(plt.rcParams.keys())\n",
    "# plt.rcParams['figure.figsize'] = (16,9)\n",
    "\n",
    "import mpld3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# reload(sdh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# reload(nn)\n",
    "import relembed as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# reload(eh)\n",
    "import experiment_helper as eh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shuffle_seed = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Data objects...\n",
      "Done creating Data objects\n",
      "85137 total examples :: 76623 training : 8514 valid (90:10 split)\n",
      "Vocab size: 23108 Dep size: 49 POS size: 18\n"
     ]
    }
   ],
   "source": [
    "reload(dh)\n",
    "DH = dh.DataHandler('data/semeval_wiki_sdp_10000', valid_percent=10, shuffle_seed=shuffle_seed) # for semeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Didn't find common ancestor\n",
      "1790\t\"The imams were removed from a US Airways <e1>flight</e1> awaiting departure from the Minneapolis-St. Paul <e2>airport</e2>.\"\n",
      "\n",
      "(The imams were removed from a US Airways flight awaiting departure from the Minneapolis - St . Paul airport ., flight , airport )\n",
      "Bad sentence: '1790\\t\"The imams were removed from a US Airways <e1>flight</e1> awaiting departure from the Minneapolis-St. Paul <e2>airport</e2>.\"\\r\\n'\n",
      "((The imams were removed from a US Airways flight awaiting departure from the Minneapolis - St . Paul airport ., flight , airport ), None)\n",
      "Skipping this one... '1790\\t\"The imams were removed from a US Airways <e1>flight</e1> awaiting departure from the Minneapolis-St. Paul <e2>airport</e2>.\"\\r\\n'\n",
      "(None, None, None, 4)\n",
      "Num training: 7199\n",
      "Num valididation: 800\n",
      "Didn't find common ancestor\n",
      "8310\t\"Tributes have been paid to the <e1>writer</e1> who created Goodness Gracious Me, the hit BBC television <e2>series</e2>.\"\n",
      "\n",
      "(Tributes have been paid to the writer who created Goodness Gracious Me , the hit BBC television series ., writer , series )\n",
      "Bad sentence: '8310\\t\"Tributes have been paid to the <e1>writer</e1> who created Goodness Gracious Me, the hit BBC television <e2>series</e2>.\"\\r\\n'\n",
      "((Tributes have been paid to the writer who created Goodness Gracious Me , the hit BBC television series ., writer , series ), None)\n",
      "Skipping this one... '8000\\t\"The <e1>surgeon</e1> cuts a small <e2>hole</e2> in the skull and lifts the edge of the brain to expose the nerve.\"\\r\\n'\n",
      "(None, None, None, 3)\n",
      "Num testing: 2717\n"
     ]
    }
   ],
   "source": [
    "# reload(sdh)\n",
    "train, valid, test, label2int, int2label = sdh.load_semeval_data(include_ends=False, shuffle_seed=shuffle_seed)\n",
    "num_classes = len(int2label.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[198, 183], [269, 20542], [4878, 262], [1047, 706], [1084, 2143]]\n"
     ]
    }
   ],
   "source": [
    "# convert the semeval data to indices under the wiki vocab:\n",
    "train['sdps'] = DH.sentences_to_sequences(train['sdps'])\n",
    "valid['sdps'] = DH.sentences_to_sequences(valid['sdps'])\n",
    "test['sdps'] = DH.sentences_to_sequences(test['sdps'])\n",
    "    \n",
    "train['targets'] = DH.sentences_to_sequences(train['targets'])\n",
    "valid['targets'] = DH.sentences_to_sequences(valid['targets'])\n",
    "test['targets'] = DH.sentences_to_sequences(test['targets'])\n",
    "\n",
    "print(train['targets'][:5]) # small sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 13\n"
     ]
    }
   ],
   "source": [
    "max_seq_len = max([len(path) for path in train['sdps']+valid['sdps']+test['sdps']])\n",
    "print(max_seq_len, DH.max_seq_len)\n",
    "DH.max_seq_len = max_seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19761 / 23108 pretrained\n"
     ]
    }
   ],
   "source": [
    "# the embedding matrix is started of as random uniform [-1,1]\n",
    "# then we replace everything but the OOV tokens with the approprate google vector\n",
    "fname = 'data/GoogleNews-vectors-negative300.bin'\n",
    "word2vec = Word2Vec.load_word2vec_format(fname, binary=True)\n",
    "\n",
    "word_embeddings = np.random.uniform(low=-1., high=1., size=[DH.vocab_size, 300]).astype(np.float32)\n",
    "num_found = 0\n",
    "for i, token in enumerate(DH.vocab):\n",
    "    if token in word2vec:\n",
    "        word_embeddings[i] = word2vec[token]\n",
    "        num_found += 1\n",
    "print(\"%i / %i pretrained\" % (num_found, DH.vocab_size))\n",
    "del word2vec # save a lot of RAM\n",
    "# normalize them\n",
    "word_embeddings /= np.sqrt(np.sum(word_embeddings**2, 1, keepdims=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def reset_drnn(model_name='relembed', bi=True, dep_embed_size=25, pos_embed_size=25, \n",
    "               word_embed_size=None, max_grad_norm=3., max_to_keep=10,\n",
    "               supervised=True, interactive=True):\n",
    "    if word_embed_size:    \n",
    "        config = {\n",
    "            'max_num_steps':DH.max_seq_len,\n",
    "            'word_embed_size':word_embed_size,\n",
    "            'dep_embed_size':dep_embed_size,\n",
    "            'pos_embed_size':pos_embed_size,\n",
    "            'bidirectional':bi,\n",
    "            'supervised':supervised,\n",
    "            'interactive':interactive,\n",
    "            'hidden_layer_size':1000,\n",
    "            'vocab_size':DH.vocab_size,\n",
    "            'dep_vocab_size':DH.dep_size,\n",
    "            'pos_vocab_size':DH.pos_size,\n",
    "            'num_predict_classes':num_classes,\n",
    "            'pretrained_word_embeddings':None,\n",
    "            'max_grad_norm':3.,\n",
    "            'model_name':model_name,\n",
    "            'max_to_keep':max_to_keep,\n",
    "            'checkpoint_prefix':'checkpoints/',\n",
    "            'summary_prefix':'tensor_summaries/'\n",
    "        }\n",
    "    else: # use pretrained google vectors\n",
    "        config = {\n",
    "            'max_num_steps':DH.max_seq_len,\n",
    "            'word_embed_size':300,\n",
    "            'dep_embed_size':dep_embed_size,\n",
    "            'pos_embed_size':pos_embed_size,\n",
    "            'bidirectional':bi,\n",
    "            'supervised':supervised,\n",
    "            'interactive':interactive,\n",
    "            'hidden_layer_size':1000,\n",
    "            'vocab_size':DH.vocab_size,\n",
    "            'dep_vocab_size':DH.dep_size,\n",
    "            'pos_vocab_size':DH.pos_size,\n",
    "            'num_predict_classes':num_classes,\n",
    "            'pretrained_word_embeddings':word_embeddings,\n",
    "            'max_grad_norm':3.,\n",
    "            'model_name':model_name,            \n",
    "            'max_to_keep':max_to_keep,\n",
    "            'checkpoint_prefix':'checkpoints/',\n",
    "            'summary_prefix':'tensor_summaries/'\n",
    "        }\n",
    "    try:\n",
    "        tf.reset_default_graph()\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        tf.get_default_session().close()\n",
    "    except:\n",
    "        pass\n",
    "    drnn = nn.RelEmbed(config)\n",
    "    print(drnn)\n",
    "    return drnn\n",
    "# drnn = reset_drnn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_validation_test(num_nearby=20):\n",
    "    valid_phrases, valid_targets , _, valid_lens,_ = DH.validation_batch()\n",
    "    random_index = int(random.uniform(0, len(valid_lens)))\n",
    "    query_phrase = valid_phrases[random_index]\n",
    "    query_len = valid_lens[random_index]\n",
    "    query_target = valid_targets[random_index].reshape((1,2))\n",
    "    padded_qp = np.zeros([DH.max_seq_len, 3]).astype(np.int32)\n",
    "    padded_qp[:len(query_phrase), 0] = [x[0] for x in query_phrase]\n",
    "    padded_qp[:len(query_phrase), 1] = [x[1] for x in query_phrase]    \n",
    "    padded_qp[:len(query_phrase), 2] = [x[2] for x in query_phrase]    \n",
    "\n",
    "    dists, phrase_idx = drnn.validation_phrase_nearby(padded_qp, query_len, query_target,\n",
    "                                                      valid_phrases, valid_lens, valid_targets)\n",
    "    print(\"=\"*80)\n",
    "    print(\"Top %i closest phrases to <%s> '%s' <%s>\" \n",
    "          % (num_nearby, DH.vocab_at(query_target[0,0]), \n",
    "             DH.sequence_to_sentence(query_phrase, query_len), \n",
    "             DH.vocab_at(query_target[0,1])))\n",
    "    for i in range(num_nearby):\n",
    "        dist = dists[i]\n",
    "        phrase = valid_phrases[phrase_idx[i]]\n",
    "        len_ = valid_lens[phrase_idx[i]]\n",
    "        target = valid_targets[phrase_idx[i]]\n",
    "        print(\"%i: %0.3f : <%s> '%s' <%s>\" \n",
    "              % (i, dist, DH.vocab_at(target[0]),\n",
    "                 DH.sequence_to_sentence(phrase, len_),\n",
    "                 DH.vocab_at(target[1])))\n",
    "    print(\"=\"*80)\n",
    "#     drnn.save_validation_accuracy(frac_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def time_left(num_epochs, num_steps, fit_time, nearby_time, start_time, nearby_mod):\n",
    "    total = num_epochs*num_steps*fit_time + ((num_epochs*num_steps)/float(nearby_mod))*nearby_time\n",
    "    return total - (time() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Unsupervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<DPNN: W:300, D:25, P:25 H:600, V:23108>\n",
      "(0:0:4) step 0/1532, epoch 0 Training Loss = 0.69158 :: 453.910 phrases/sec :: (0:30:51) hours left\n",
      "================================================================================\n",
      "Top 50 closest phrases to <bruce> '<X> by attacked in <Y>' <south>\n",
      "0: 1.000 : <bruce> '<X> by attacked in <Y>' <south>\n",
      "1: 0.947 : <base> '<X> attacked in <Y>' <battle>\n",
      "2: 0.886 : <goal> '<X> by followed in <Y>' <march>\n",
      "3: 0.875 : <stadium> '<X> at defeated in <Y>' <front>\n",
      "4: 0.872 : <states> '<X> in in <Y>' <midwest>\n",
      "5: 0.872 : <bay> '<X> in in <Y>' <locations>\n",
      "6: 0.872 : <florida> '<X> in in <Y>' <u.s.>\n",
      "7: 0.872 : <europe> '<X> in in <Y>' <pacific>\n",
      "8: 0.869 : <he> '<X> defeated in <Y>' <rounds>\n",
      "9: 0.867 : <rose> '<X> was in <Y>' <band>\n",
      "10: 0.867 : <appointment> '<X> in <Y>' <favour>\n",
      "11: 0.867 : <time> '<X> in <Y>' <london>\n",
      "12: 0.867 : <acid> '<X> in <Y>' <reactions>\n",
      "13: 0.867 : <louvre> '<X> in <Y>' <paris>\n",
      "14: 0.867 : <term> '<X> in <Y>' <election>\n",
      "15: 0.867 : <offices> '<X> in <Y>' <boston>\n",
      "16: 0.867 : <degree> '<X> in <Y>' <geology>\n",
      "17: 0.867 : <outbreak> '<X> in <Y>' <year>\n",
      "18: 0.867 : <talent> '<X> in <Y>' <companies>\n",
      "19: 0.867 : <policeman> '<X> in <Y>' <department>\n",
      "20: 0.867 : <districts> '<X> in <Y>' <region>\n",
      "21: 0.867 : <students> '<X> in <Y>' <grades>\n",
      "22: 0.867 : <family> '<X> in <Y>' <sicily>\n",
      "23: 0.867 : <models> '<X> in <Y>' <island>\n",
      "24: 0.867 : <males> '<X> in <Y>' <division>\n",
      "25: 0.867 : <nationals> '<X> in <Y>' <adelaide>\n",
      "26: 0.867 : <attorney> '<X> in <Y>' <practice>\n",
      "27: 0.867 : <disguise> '<X> in <Y>' <festival>\n",
      "28: 0.867 : <subsidies> '<X> in <Y>' <states>\n",
      "29: 0.867 : <struggle> '<X> in <Y>' <cape>\n",
      "30: 0.867 : <rates> '<X> in <Y>' <country>\n",
      "31: 0.867 : <hotel> '<X> in <Y>' <city>\n",
      "32: 0.867 : <wins> '<X> in <Y>' <series>\n",
      "33: 0.867 : <university> '<X> in <Y>' <mississippi>\n",
      "34: 0.867 : <property> '<X> in <Y>' <township>\n",
      "35: 0.867 : <hamlet> '<X> in <Y>' <part>\n",
      "36: 0.867 : <conclusion> '<X> in <Y>' <pacific>\n",
      "37: 0.867 : <vehicles> '<X> in <Y>' <europe>\n",
      "38: 0.867 : <students> '<X> in <Y>' <residence>\n",
      "39: 0.867 : <hall> '<X> in <Y>' <park>\n",
      "40: 0.867 : <system> '<X> in <Y>' <massachusetts>\n",
      "41: 0.867 : <creeks> '<X> in <Y>' <hills>\n",
      "42: 0.867 : <avalanche> '<X> in <Y>' <draft>\n",
      "43: 0.867 : <change> '<X> in <Y>' <flow>\n",
      "44: 0.867 : <home> '<X> in <Y>' <thame>\n",
      "45: 0.867 : <church> '<X> in <Y>' <america>\n",
      "46: 0.867 : <dormitory> '<X> in <Y>' <vernon>\n",
      "47: 0.867 : <contamination> '<X> in <Y>' <city>\n",
      "48: 0.867 : <cemetery> '<X> in <Y>' <denver>\n",
      "49: 0.867 : <actions> '<X> in <Y>' <interview>\n",
      "================================================================================\n",
      "Validation loss: 0.6819\n",
      "(0:0:44) step 10/1532, epoch 0 Training Loss = 0.67251 :: 308.913 phrases/sec :: (0:46:3) hours left\n",
      "(0:1:6) step 20/1532, epoch 0 Training Loss = 0.65252 :: 281.144 phrases/sec :: (0:50:11) hours left\n",
      "(0:1:24) step 30/1532, epoch 0 Training Loss = 0.62368 :: 291.281 phrases/sec :: (0:48:9) hours left\n",
      "(0:1:43) step 40/1532, epoch 0 Training Loss = 0.58479 :: 289.773 phrases/sec :: (0:48:5) hours left\n",
      "(0:2:5) step 50/1532, epoch 0 Training Loss = 0.52869 :: 280.221 phrases/sec :: (0:49:21) hours left\n",
      "(0:2:23) step 60/1532, epoch 0 Training Loss = 0.47037 :: 285.311 phrases/sec :: (0:48:10) hours left\n",
      "(0:2:43) step 70/1532, epoch 0 Training Loss = 0.39627 :: 284.878 phrases/sec :: (0:47:55) hours left\n",
      "(0:3:1) step 80/1532, epoch 0 Training Loss = 0.40502 :: 286.668 phrases/sec :: (0:47:18) hours left\n",
      "(0:3:20) step 90/1532, epoch 0 Training Loss = 0.39595 :: 287.597 phrases/sec :: (0:46:50) hours left\n",
      "(0:3:40) step 100/1532, epoch 0 Training Loss = 0.37047 :: 285.510 phrases/sec :: (0:46:51) hours left\n",
      "(0:3:59) step 110/1532, epoch 0 Training Loss = 0.36760 :: 286.996 phrases/sec :: (0:46:17) hours left\n",
      "(0:4:19) step 120/1532, epoch 0 Training Loss = 0.35510 :: 285.283 phrases/sec :: (0:46:14) hours left\n",
      "(0:4:40) step 130/1532, epoch 0 Training Loss = 0.34468 :: 283.543 phrases/sec :: (0:46:11) hours left\n",
      "(0:4:59) step 140/1532, epoch 0 Training Loss = 0.34404 :: 284.196 phrases/sec :: (0:45:46) hours left\n",
      "(0:5:19) step 150/1532, epoch 0 Training Loss = 0.34161 :: 283.225 phrases/sec :: (0:45:35) hours left\n",
      "(0:5:41) step 160/1532, epoch 0 Training Loss = 0.33623 :: 280.963 phrases/sec :: (0:45:37) hours left\n",
      "(0:5:59) step 170/1532, epoch 0 Training Loss = 0.34634 :: 282.823 phrases/sec :: (0:45:0) hours left\n",
      "(0:6:19) step 180/1532, epoch 0 Training Loss = 0.33590 :: 282.078 phrases/sec :: (0:44:48) hours left\n",
      "(0:6:41) step 190/1532, epoch 0 Training Loss = 0.32695 :: 280.879 phrases/sec :: (0:44:39) hours left\n",
      "(0:7:2) step 200/1532, epoch 0 Training Loss = 0.32804 :: 279.901 phrases/sec :: (0:44:28) hours left\n",
      "================================================================================\n",
      "Top 50 closest phrases to <germans> '<X> began <OOV> of <Y>' <jews>\n",
      "0: 1.000 : <germans> '<X> began <OOV> of <Y>' <jews>\n",
      "1: 1.000 : <physician> '<X> includes <OOV> of <Y>' <lioness>\n",
      "2: 1.000 : <father> '<X> was <OOV> of <Y>' <clan>\n",
      "3: 1.000 : <people> '<X> were <OOV> of <Y>' <population>\n",
      "4: 1.000 : <it> '<X> was <OOV> of <Y>' <hopefuls>\n",
      "5: 1.000 : <part> '<X> of <OOV> of <Y>' <switzerland>\n",
      "6: 1.000 : <proposals> '<X> including <OOV> of <Y>' <objects>\n",
      "7: 1.000 : <control> '<X> of <OOV> of <Y>' <wales>\n",
      "8: 1.000 : <districts> '<X> cities <OOV> of <Y>' <schools>\n",
      "9: 1.000 : <land> '<X> consists of <OOV> <Y>' <olive>\n",
      "10: 1.000 : <preparation> '<X> for <OOV> of <Y>' <facility>\n",
      "11: 1.000 : <it> '<X> <OOV> number of <Y>' <times>\n",
      "12: 1.000 : <analysis> '<X> <OOV> number of <Y>' <bodies>\n",
      "13: 1.000 : <express> '<X> <OOV> nation of <Y>' <immigration>\n",
      "14: 1.000 : <photo> '<X> took out of <Y>' <window>\n",
      "15: 1.000 : <wear> '<X> <OOV> areas of <Y>' <yorkshire>\n",
      "16: 1.000 : <measure> '<X> of <OOV> in <Y>' <interior>\n",
      "17: 1.000 : <plans> '<X> of <OOV> throughout <Y>' <war>\n",
      "18: 1.000 : <he> '<X> joined school of <Y>' <art>\n",
      "19: 1.000 : <he> '<X> took wicket of <Y>' <lewis>\n",
      "20: 1.000 : <america> '<X> <OOV> king of <Y>' <islands>\n",
      "21: 1.000 : <club> '<X> grew out of <Y>' <union>\n",
      "22: 1.000 : <municipality> '<X> of <OOV> in <Y>' <brussels>\n",
      "23: 1.000 : <society> '<X> of member <OOV> <Y>' <others>\n",
      "24: 1.000 : <variety> '<X> of <OOV> including <Y>' <plover>\n",
      "25: 1.000 : <world> '<X> of rest <OOV> <Y>' <usa>\n",
      "26: 1.000 : <gays> '<X> come out of <Y>' <closet>\n",
      "27: 1.000 : <juniors> '<X> of <OOV> were <Y>' <club>\n",
      "28: 1.000 : <cases> '<X> in consist of <Y>' <combination>\n",
      "29: 1.000 : <phase> '<X> saw fleet of <Y>' <taxis>\n",
      "30: 1.000 : <it> '<X> was site of <Y>' <store>\n",
      "31: 0.999 : <elements> '<X> were out of <Y>' <control>\n",
      "32: 0.999 : <ryan> '<X> held position of <Y>' <minister>\n",
      "33: 0.999 : <benefits> '<X> gain out of <Y>' <it>\n",
      "34: 0.999 : <center> '<X> of oscillation <OOV> <Y>' <laws>\n",
      "35: 0.999 : <man> '<X> sends out of <Y>' <house>\n",
      "36: 0.999 : <choice> '<X> of names <OOV> <Y>' <details>\n",
      "37: 0.999 : <commander> '<X> of order of <Y>' <empire>\n",
      "38: 0.999 : <chapters> '<X> of <OOV> provide <Y>' <episode>\n",
      "39: 0.999 : <he> '<X> became principal of <Y>' <school>\n",
      "40: 0.999 : <rocks> '<X> of part of <Y>' <belt>\n",
      "41: 0.999 : <she> '<X> became president of <Y>' <rock>\n",
      "42: 0.999 : <members> '<X> of forces of <Y>' <colombia>\n",
      "43: 0.999 : <he> '<X> was chair of <Y>' <judges>\n",
      "44: 0.999 : <university> '<X> of <OOV> studied <Y>' <he>\n",
      "45: 0.999 : <vermont> '<X> experienced day of <Y>' <war>\n",
      "46: 0.999 : <stone> '<X> is built of <Y>' <brick>\n",
      "47: 0.999 : <bills> '<X> make half of <Y>' <bankruptcies>\n",
      "48: 0.999 : <beam> '<X> of action <OOV> <Y>' <beam>\n",
      "49: 0.999 : <registration> '<X> is registration of <Y>' <issue>\n",
      "================================================================================\n",
      "Validation loss: 2.3311\n",
      "(0:7:52) step 210/1532, epoch 0 Training Loss = 0.33061 :: 277.932 phrases/sec :: (0:44:0) hours left\n",
      "(0:8:12) step 220/1532, epoch 0 Training Loss = 0.32735 :: 277.470 phrases/sec :: (0:43:45) hours left\n",
      "(0:8:37) step 230/1532, epoch 0 Training Loss = 0.33254 :: 274.546 phrases/sec :: (0:43:52) hours left\n",
      "(0:9:0) step 240/1532, epoch 0 Training Loss = 0.33227 :: 273.090 phrases/sec :: (0:43:46) hours left\n",
      "(0:9:22) step 250/1532, epoch 0 Training Loss = 0.32397 :: 272.373 phrases/sec :: (0:43:33) hours left\n",
      "(0:9:44) step 260/1532, epoch 0 Training Loss = 0.32744 :: 271.352 phrases/sec :: (0:43:22) hours left\n",
      "(0:10:5) step 270/1532, epoch 0 Training Loss = 0.33003 :: 270.772 phrases/sec :: (0:43:7) hours left\n",
      "(0:10:33) step 280/1532, epoch 0 Training Loss = 0.32394 :: 267.585 phrases/sec :: (0:43:17) hours left\n",
      "(0:10:58) step 290/1532, epoch 0 Training Loss = 0.32556 :: 265.480 phrases/sec :: (0:43:16) hours left\n",
      "(0:11:18) step 300/1532, epoch 0 Training Loss = 0.32462 :: 265.844 phrases/sec :: (0:42:52) hours left\n",
      "(0:11:41) step 310/1532, epoch 0 Training Loss = 0.32709 :: 264.868 phrases/sec :: (0:42:40) hours left\n",
      "(0:12:0) step 320/1532, epoch 0 Training Loss = 0.32333 :: 265.548 phrases/sec :: (0:42:13) hours left\n",
      "(0:12:21) step 330/1532, epoch 0 Training Loss = 0.32296 :: 265.525 phrases/sec :: (0:41:53) hours left\n",
      "(0:12:44) step 340/1532, epoch 0 Training Loss = 0.32640 :: 264.911 phrases/sec :: (0:41:38) hours left\n",
      "(0:13:1) step 350/1532, epoch 0 Training Loss = 0.32428 :: 266.031 phrases/sec :: (0:41:6) hours left\n",
      "(0:13:23) step 360/1532, epoch 0 Training Loss = 0.32151 :: 265.789 phrases/sec :: (0:40:48) hours left\n",
      "(0:13:45) step 370/1532, epoch 0 Training Loss = 0.32159 :: 265.460 phrases/sec :: (0:40:30) hours left\n",
      "(0:14:4) step 380/1532, epoch 0 Training Loss = 0.32155 :: 265.875 phrases/sec :: (0:40:5) hours left\n",
      "(0:14:27) step 390/1532, epoch 0 Training Loss = 0.32423 :: 265.362 phrases/sec :: (0:39:49) hours left\n",
      "(0:14:50) step 400/1532, epoch 0 Training Loss = 0.32386 :: 264.456 phrases/sec :: (0:39:36) hours left\n",
      "================================================================================\n",
      "Top 50 closest phrases to <valley> '<X> is shaped narrowing <Y>' <north>\n",
      "0: 1.000 : <valley> '<X> is shaped narrowing <Y>' <north>\n",
      "1: 0.999 : <he> '<X> is horror filling <Y>' <square>\n",
      "2: 0.999 : <ashes> '<X> with landscape is <Y>' <painting>\n",
      "3: 0.999 : <habit> '<X> developed <OOV> is <Y>' <feature>\n",
      "4: 0.999 : <ecophilosophy> '<X> of aim is <Y>' <view>\n",
      "5: 0.999 : <context> '<X> in viewed is <Y>' <system>\n",
      "6: 0.999 : <terminus> '<X> of gauge is <Y>' <railway>\n",
      "7: 0.999 : <june> '<X> in opened is <Y>' <prison>\n",
      "8: 0.999 : <nations> '<X> to manufacturer is <Y>' <supplier>\n",
      "9: 0.999 : <requirements> '<X> for verifying is <Y>' <player>\n",
      "10: 0.999 : <world> '<X> getting been defines <Y>' <james>\n",
      "11: 0.999 : <terms> '<X> by is lay <Y>' <gauge>\n",
      "12: 0.999 : <west> '<X> on succeeding is <Y>' <range>\n",
      "13: 0.999 : <it> '<X> is driving scales <Y>' <connection>\n",
      "14: 0.999 : <war> '<X> with made is <Y>' <it>\n",
      "15: 0.999 : <wind> '<X> in promises is <Y>' <novel>\n",
      "16: 0.999 : <permission> '<X> grants is on <Y>' <verge>\n",
      "17: 0.999 : <trespass> '<X> of gist is <Y>' <interference>\n",
      "18: 0.999 : <australia> '<X> of council is <Y>' <association>\n",
      "19: 0.999 : <editor> '<X> of adaptation is <Y>' <editor>\n",
      "20: 0.999 : <fragments> '<X> of law is <Y>' <method>\n",
      "21: 0.999 : <families> '<X> from are is <Y>' <school>\n",
      "22: 0.999 : <format> '<X> is contains through <Y>' <grade>\n",
      "23: 0.999 : <bit> '<X> be expected is <Y>' <pi>\n",
      "24: 0.999 : <loan> '<X> secure trying is <Y>' <witness>\n",
      "25: 0.999 : <station> '<X> is administered from <Y>' <station>\n",
      "26: 0.999 : <church> '<X> be thought includes <Y>' <stonework>\n",
      "27: 0.999 : <shade> '<X> above <OOV> is <Y>' <flower>\n",
      "28: 0.999 : <compact> '<X> is on running <Y>' <android>\n",
      "29: 0.999 : <machine> '<X> on assembly contains <Y>' <pins>\n",
      "30: 0.999 : <ship> '<X> board see is <Y>' <one>\n",
      "31: 0.999 : <peace> '<X> of clerk is <Y>' <head>\n",
      "32: 0.999 : <martin> '<X> is safety is <Y>' <who>\n",
      "33: 0.999 : <tail> '<X> has claw on <Y>' <foot>\n",
      "34: 0.999 : <shade> '<X> above <OOV> is <Y>' <feature>\n",
      "35: 0.999 : <coordinator> '<X> at university is <Y>' <he>\n",
      "36: 0.999 : <coordinator> '<X> at university is <Y>' <director>\n",
      "37: 0.999 : <stone> '<X> is built of <Y>' <brick>\n",
      "38: 0.999 : <church> '<X> be thought includes <Y>' <it>\n",
      "39: 0.999 : <it> '<X> is game takes <Y>' <player>\n",
      "40: 0.999 : <player> '<X> is within of <Y>' <age>\n",
      "41: 0.999 : <radio> '<X> is album by <Y>' <thornton>\n",
      "42: 0.999 : <nave> '<X> comprises thought includes <Y>' <it>\n",
      "43: 0.999 : <area> '<X> forming made within <Y>' <county>\n",
      "44: 0.999 : <use> '<X> reflect updated is <Y>' <compact>\n",
      "45: 0.999 : <shell> '<X> hook via is <Y>' <it>\n",
      "46: 0.999 : <violinist> '<X> produces by drawing <Y>' <bow>\n",
      "47: 0.999 : <change> '<X> is switch from <Y>' <turbines>\n",
      "48: 0.999 : <abdomen> '<X> <OOV> head are <Y>' <exception>\n",
      "49: 0.999 : <solutions> '<X> provides in arabia <Y>' <east>\n",
      "================================================================================\n",
      "Validation loss: 2.3633\n",
      "(0:15:41) step 410/1532, epoch 0 Training Loss = 0.32372 :: 263.236 phrases/sec :: (0:39:1) hours left\n",
      "(0:16:5) step 420/1532, epoch 0 Training Loss = 0.32231 :: 262.469 phrases/sec :: (0:38:46) hours left\n",
      "(0:16:24) step 430/1532, epoch 0 Training Loss = 0.32375 :: 262.929 phrases/sec :: (0:38:21) hours left\n",
      "(0:16:46) step 440/1532, epoch 0 Training Loss = 0.32204 :: 262.703 phrases/sec :: (0:38:2) hours left\n",
      "(0:17:9) step 450/1532, epoch 0 Training Loss = 0.32326 :: 262.188 phrases/sec :: (0:37:46) hours left\n",
      "(0:17:29) step 460/1532, epoch 0 Training Loss = 0.32114 :: 262.321 phrases/sec :: (0:37:23) hours left\n",
      "(0:17:53) step 470/1532, epoch 0 Training Loss = 0.32301 :: 261.576 phrases/sec :: (0:37:9) hours left\n",
      "(0:18:19) step 480/1532, epoch 0 Training Loss = 0.32223 :: 260.415 phrases/sec :: (0:36:57) hours left\n",
      "(0:18:40) step 490/1532, epoch 0 Training Loss = 0.32008 :: 260.467 phrases/sec :: (0:36:36) hours left\n",
      "(0:19:3) step 500/1532, epoch 0 Training Loss = 0.32088 :: 259.958 phrases/sec :: (0:36:19) hours left\n",
      "(0:19:27) step 510/1532, epoch 0 Training Loss = 0.32297 :: 259.457 phrases/sec :: (0:36:2) hours left\n",
      "(0:19:47) step 520/1532, epoch 0 Training Loss = 0.32300 :: 259.599 phrases/sec :: (0:35:39) hours left\n",
      "(0:20:15) step 530/1532, epoch 0 Training Loss = 0.32254 :: 258.196 phrases/sec :: (0:35:29) hours left\n",
      "(0:20:39) step 540/1532, epoch 0 Training Loss = 0.32237 :: 257.570 phrases/sec :: (0:35:13) hours left\n",
      "(0:20:59) step 550/1532, epoch 0 Training Loss = 0.32012 :: 257.908 phrases/sec :: (0:34:49) hours left\n",
      "(0:21:22) step 560/1532, epoch 0 Training Loss = 0.32178 :: 257.543 phrases/sec :: (0:34:30) hours left\n",
      "(0:21:43) step 570/1532, epoch 0 Training Loss = 0.31797 :: 257.706 phrases/sec :: (0:34:8) hours left\n",
      "(0:22:2) step 580/1532, epoch 0 Training Loss = 0.32210 :: 258.117 phrases/sec :: (0:33:43) hours left\n",
      "(0:22:26) step 590/1532, epoch 0 Training Loss = 0.32254 :: 257.556 phrases/sec :: (0:33:26) hours left\n",
      "(0:22:47) step 600/1532, epoch 0 Training Loss = 0.31966 :: 257.673 phrases/sec :: (0:33:3) hours left\n",
      "================================================================================\n",
      "Top 50 closest phrases to <students> '<X> moving into <Y>' <components>\n",
      "0: 1.000 : <students> '<X> moving into <Y>' <components>\n",
      "1: 1.000 : <model> '<X> moving into <Y>' <territory>\n",
      "2: 1.000 : <country> '<X> moved into <Y>' <century>\n",
      "3: 1.000 : <output> '<X> running into <Y>' <barriers>\n",
      "4: 1.000 : <workers> '<X> pushed into <Y>' <depression>\n",
      "5: 1.000 : <religion> '<X> spreading into <Y>' <country>\n",
      "6: 1.000 : <attendee> '<X> journeyed into <Y>' <concepts>\n",
      "7: 1.000 : <gallon> '<X> put into <Y>' <cart>\n",
      "8: 1.000 : <plan> '<X> ran into <Y>' <resistance>\n",
      "9: 1.000 : <deers> '<X> ran into <Y>' <cars>\n",
      "10: 1.000 : <boy> '<X> went into <Y>' <bathroom>\n",
      "11: 1.000 : <shot> '<X> put into <Y>' <bunker>\n",
      "12: 1.000 : <profit> '<X> brought into <Y>' <company>\n",
      "13: 1.000 : <forests> '<X> converting into <Y>' <fields>\n",
      "14: 1.000 : <creativity> '<X> puts into <Y>' <use>\n",
      "15: 1.000 : <prisoner> '<X> departed into <Y>' <captivity>\n",
      "16: 1.000 : <they> '<X> put into <Y>' <tree>\n",
      "17: 1.000 : <temples> '<X> fallen into <Y>' <disrepair>\n",
      "18: 1.000 : <star> '<X> landing into <Y>' <controversies>\n",
      "19: 1.000 : <shells> '<X> landed into <Y>' <areas>\n",
      "20: 1.000 : <students> '<X> got into <Y>' <fight>\n",
      "21: 1.000 : <sewage> '<X> dumped into <Y>' <river>\n",
      "22: 1.000 : <intestines> '<X> migrated into <Y>' <abdomen>\n",
      "23: 1.000 : <student> '<X> fell into <Y>' <river>\n",
      "24: 1.000 : <victim> '<X> dragged into <Y>' <water>\n",
      "25: 1.000 : <germans> '<X> broken into <Y>' <pockets>\n",
      "26: 1.000 : <heads> '<X> turns into <Y>' <park>\n",
      "27: 1.000 : <remains> '<X> dumped into <Y>' <pool>\n",
      "28: 1.000 : <troops> '<X> piled into <Y>' <hill>\n",
      "29: 1.000 : <hikers> '<X> piled into <Y>' <shelters>\n",
      "30: 1.000 : <sediments> '<X> transported into <Y>' <lake>\n",
      "31: 1.000 : <saltpetre> '<X> packed into <Y>' <sticks>\n",
      "32: 1.000 : <molding> '<X> blew into <Y>' <paperboard>\n",
      "33: 1.000 : <seeds> '<X> blown into <Y>' <fields>\n",
      "34: 1.000 : <prisoner> '<X> taken into <Y>' <custody>\n",
      "35: 1.000 : <parties> '<X> taken into <Y>' <confidence>\n",
      "36: 1.000 : <catheter> '<X> placed into <Y>' <stomach>\n",
      "37: 1.000 : <implants> '<X> placed into <Y>' <jawbone>\n",
      "38: 1.000 : <fame> '<X> spread into <Y>' <egypt>\n",
      "39: 1.000 : <he> '<X> grew into <Y>' <role>\n",
      "40: 1.000 : <dollars> '<X> flow into <Y>' <facilities>\n",
      "41: 1.000 : <it> '<X> blends into <Y>' <approach>\n",
      "42: 1.000 : <employee> '<X> thrown into <Y>' <issues>\n",
      "43: 1.000 : <metres> '<X> reached into <Y>' <air>\n",
      "44: 0.999 : <coins> '<X> throw into <Y>' <fountain>\n",
      "45: 0.999 : <money> '<X> poured into <Y>' <schools>\n",
      "46: 0.999 : <billions> '<X> poured into <Y>' <safety>\n",
      "47: 0.999 : <fluid> '<X> leaked into <Y>' <cuffs>\n",
      "48: 0.999 : <fuel> '<X> leaked into <Y>' <crankcase>\n",
      "49: 0.999 : <cancer> '<X> spreads into <Y>' <areas>\n",
      "================================================================================\n",
      "Validation loss: 2.3759\n",
      "(0:23:37) step 610/1532, epoch 0 Training Loss = 0.32132 :: 258.109 phrases/sec :: (0:32:9) hours left\n",
      "(0:24:0) step 620/1532, epoch 0 Training Loss = 0.32122 :: 257.777 phrases/sec :: (0:31:50) hours left\n",
      "(0:24:23) step 630/1532, epoch 0 Training Loss = 0.31933 :: 257.452 phrases/sec :: (0:31:31) hours left\n",
      "(0:24:44) step 640/1532, epoch 0 Training Loss = 0.31926 :: 257.566 phrases/sec :: (0:31:9) hours left\n",
      "(0:25:8) step 650/1532, epoch 0 Training Loss = 0.31968 :: 256.984 phrases/sec :: (0:30:52) hours left\n",
      "(0:25:29) step 660/1532, epoch 0 Training Loss = 0.32157 :: 257.087 phrases/sec :: (0:30:29) hours left\n",
      "(0:25:50) step 670/1532, epoch 0 Training Loss = 0.31899 :: 257.269 phrases/sec :: (0:30:7) hours left\n",
      "(0:26:13) step 680/1532, epoch 0 Training Loss = 0.32000 :: 256.870 phrases/sec :: (0:29:48) hours left\n",
      "(0:26:33) step 690/1532, epoch 0 Training Loss = 0.32077 :: 257.288 phrases/sec :: (0:29:23) hours left\n",
      "(0:26:53) step 700/1532, epoch 0 Training Loss = 0.31934 :: 257.493 phrases/sec :: (0:29:1) hours left\n",
      "(0:27:15) step 710/1532, epoch 0 Training Loss = 0.32231 :: 257.403 phrases/sec :: (0:28:40) hours left\n",
      "(0:27:34) step 720/1532, epoch 0 Training Loss = 0.32123 :: 257.824 phrases/sec :: (0:28:15) hours left\n",
      "(0:27:55) step 730/1532, epoch 0 Training Loss = 0.32064 :: 257.796 phrases/sec :: (0:27:54) hours left\n",
      "(0:28:18) step 740/1532, epoch 0 Training Loss = 0.31955 :: 257.660 phrases/sec :: (0:27:34) hours left\n",
      "(0:28:37) step 750/1532, epoch 0 Training Loss = 0.32211 :: 258.016 phrases/sec :: (0:27:10) hours left\n",
      "(0:28:58) step 760/1532, epoch 0 Training Loss = 0.31840 :: 258.004 phrases/sec :: (0:26:49) hours left\n",
      "(0:29:22) step 770/1532, epoch 0 Training Loss = 0.31786 :: 257.701 phrases/sec :: (0:26:29) hours left\n",
      "(0:29:46) step 780/1532, epoch 0 Training Loss = 0.31991 :: 257.322 phrases/sec :: (0:26:10) hours left\n",
      "Saving model...\n",
      "Saving model to file: checkpoints/renormalize.ckpt-789-0\n",
      "(0:30:9) step 790/1532, epoch 0 Training Loss = 0.32116 :: 257.665 phrases/sec :: (0:25:42) hours left\n",
      "(0:30:30) step 800/1532, epoch 0 Training Loss = 0.31974 :: 257.857 phrases/sec :: (0:25:19) hours left\n",
      "================================================================================\n",
      "Top 50 closest phrases to <hand> '<X> on appreciate stay for <Y>' <days>\n",
      "0: 1.000 : <hand> '<X> on appreciate stay for <Y>' <days>\n",
      "1: 0.999 : <members> '<X> take create conditions for <Y>' <astronomy>\n",
      "2: 0.999 : <income> '<X> put into feedback for <Y>' <treatments>\n",
      "3: 0.999 : <it> '<X> is adjusted support for <Y>' <transit>\n",
      "4: 0.999 : <understanding> '<X> increase highlight allow for <Y>' <scores>\n",
      "5: 0.999 : <days> '<X> for stay <OOV> with <Y>' <purchases>\n",
      "6: 0.999 : <hand> '<X> on <OOV> reacts for <Y>' <example>\n",
      "7: 0.999 : <situation> '<X> be intended searched for <Y>' <replacement>\n",
      "8: 0.999 : <government> '<X> by promises draw for <Y>' <region>\n",
      "9: 0.999 : <september> '<X> in decided <OOV> for <Y>' <casino>\n",
      "10: 0.999 : <he> '<X> started as starter for <Y>' <cubs>\n",
      "11: 0.999 : <companies> '<X> organized fight for for <Y>' <defense>\n",
      "12: 0.999 : <businesses> '<X> appreciate stay for <Y>' <days>\n",
      "13: 0.999 : <nationals> '<X> required give name for <Y>' <passport>\n",
      "14: 0.999 : <shuttle> '<X> went through preparations for <Y>' <launch>\n",
      "15: 0.999 : <record> '<X> set by playing for <Y>' <clubs>\n",
      "16: 0.999 : <predecessor> '<X> as numbers went for <Y>' <years>\n",
      "17: 0.999 : <it> '<X> served as clubhouse for <Y>' <club>\n",
      "18: 0.999 : <society> '<X> of award won for <Y>' <star>\n",
      "19: 0.999 : <bill> '<X> requires review look for <Y>' <ways>\n",
      "20: 0.999 : <minutes> '<X> hours was used for <Y>' <release>\n",
      "21: 0.999 : <concussion> '<X> sent securing choice for <Y>' <chiefs>\n",
      "22: 0.999 : <stout> '<X> serve agreed held for <Y>' <term>\n",
      "23: 0.999 : <buttons> '<X> wheels of pair for <Y>' <drive>\n",
      "24: 0.999 : <francis> '<X> <OOV> for use as <Y>' <apartment>\n",
      "25: 0.999 : <themselves> '<X> <OOV> passed otherwise for <Y>' <police>\n",
      "26: 0.999 : <seawater> '<X> in deep is for <Y>' <half>\n",
      "27: 0.999 : <track> '<X> <OOV> form park for <Y>' <use>\n",
      "28: 0.999 : <opposition> '<X> facing decided <OOV> for <Y>' <casino>\n",
      "29: 0.999 : <states> '<X> of center calculated for <Y>' <census>\n",
      "30: 0.999 : <august> '<X> in had shortened for <Y>' <behavior>\n",
      "31: 0.999 : <air> '<X> into reached carrying for <Y>' <distance>\n",
      "32: 0.999 : <regions> '<X> from administrators was for <Y>' <communists>\n",
      "33: 0.999 : <school> '<X> at taught implemented for <Y>' <america>\n",
      "34: 0.999 : <title> '<X> for intelligence beat on <Y>' <august>\n",
      "35: 0.999 : <ross> '<X> asks kill in exchange for <Y>' <ross>\n",
      "36: 0.999 : <cup> '<X> as known cup for <Y>' <reasons>\n",
      "37: 0.999 : <mayor> '<X> for run decides dalton for <Y>' <sheriff>\n",
      "38: 0.999 : <act> '<X> of passage adjusted for <Y>' <inflation>\n",
      "39: 0.999 : <november> '<X> in entered served for <Y>' <city>\n",
      "40: 0.999 : <december> '<X> on <OOV> join movement for <Y>' <justice>\n",
      "41: 0.999 : <example> '<X> of style used for <Y>' <buildings>\n",
      "42: 0.999 : <magazine> '<X> recognized as <NUM> for <Y>' <years>\n",
      "43: 0.999 : <isis> '<X> <OOV> saying pleaded for <Y>' <release>\n",
      "44: 0.999 : <whom> '<X> of many stay at <Y>' <time>\n",
      "45: 0.999 : <passports> '<X> for forms print for <Y>' <completion>\n",
      "46: 0.999 : <he> '<X> known for posting on <Y>' <channel>\n",
      "47: 0.999 : <union> '<X> of consisted <OOV> for <Y>' <program>\n",
      "48: 0.999 : <engine> '<X> received block <OOV> for <Y>' <reduction>\n",
      "49: 0.999 : <agreement> '<X> without implement be in <Y>' <case>\n",
      "================================================================================\n",
      "Validation loss: 2.3875\n",
      "(0:31:18) step 810/1532, epoch 0 Training Loss = 0.31985 :: 258.066 phrases/sec :: (0:24:29) hours left\n",
      "(0:31:39) step 820/1532, epoch 0 Training Loss = 0.31965 :: 258.080 phrases/sec :: (0:24:7) hours left\n",
      "(0:32:0) step 830/1532, epoch 0 Training Loss = 0.31807 :: 258.071 phrases/sec :: (0:23:46) hours left\n",
      "(0:32:25) step 840/1532, epoch 0 Training Loss = 0.31759 :: 257.682 phrases/sec :: (0:23:27) hours left\n",
      "(0:32:49) step 850/1532, epoch 0 Training Loss = 0.31740 :: 257.299 phrases/sec :: (0:23:7) hours left\n",
      "(0:33:9) step 860/1532, epoch 0 Training Loss = 0.32048 :: 257.427 phrases/sec :: (0:22:45) hours left\n",
      "(0:33:32) step 870/1532, epoch 0 Training Loss = 0.31854 :: 257.280 phrases/sec :: (0:22:24) hours left\n",
      "(0:33:54) step 880/1532, epoch 0 Training Loss = 0.31897 :: 257.243 phrases/sec :: (0:22:3) hours left\n",
      "(0:34:17) step 890/1532, epoch 0 Training Loss = 0.31743 :: 256.997 phrases/sec :: (0:21:43) hours left\n",
      "(0:34:40) step 900/1532, epoch 0 Training Loss = 0.31811 :: 256.714 phrases/sec :: (0:21:23) hours left\n",
      "(0:35:5) step 910/1532, epoch 0 Training Loss = 0.31535 :: 256.363 phrases/sec :: (0:21:3) hours left\n",
      "(0:35:30) step 920/1532, epoch 0 Training Loss = 0.31831 :: 255.875 phrases/sec :: (0:20:44) hours left\n",
      "(0:35:54) step 930/1532, epoch 0 Training Loss = 0.31860 :: 255.530 phrases/sec :: (0:20:24) hours left\n",
      "(0:36:19) step 940/1532, epoch 0 Training Loss = 0.31777 :: 255.133 phrases/sec :: (0:20:5) hours left\n",
      "(0:36:44) step 950/1532, epoch 0 Training Loss = 0.31763 :: 254.707 phrases/sec :: (0:19:45) hours left\n",
      "(0:37:6) step 960/1532, epoch 0 Training Loss = 0.31892 :: 254.724 phrases/sec :: (0:19:23) hours left\n",
      "(0:37:32) step 970/1532, epoch 0 Training Loss = 0.31619 :: 254.136 phrases/sec :: (0:19:5) hours left\n",
      "(0:37:57) step 980/1532, epoch 0 Training Loss = 0.31917 :: 253.712 phrases/sec :: (0:18:45) hours left\n",
      "(0:38:23) step 990/1532, epoch 0 Training Loss = 0.32058 :: 253.268 phrases/sec :: (0:18:25) hours left\n",
      "(0:38:51) step 1000/1532, epoch 0 Training Loss = 0.31724 :: 252.522 phrases/sec :: (0:18:7) hours left\n",
      "================================================================================\n",
      "Top 50 closest phrases to <separation> '<X> retreated following <Y>' <defeat>\n",
      "0: 1.000 : <separation> '<X> retreated following <Y>' <defeat>\n",
      "1: 1.000 : <match> '<X> following following <Y>' <match>\n",
      "2: 1.000 : <duties> '<X> continued following <Y>' <replacement>\n",
      "3: 0.999 : <split> '<X> gave following <Y>' <independence>\n",
      "4: 0.999 : <staff> '<X> joining following <Y>' <hiring>\n",
      "5: 0.999 : <greece> '<X> applied following <Y>' <restoration>\n",
      "6: 0.999 : <engagement> '<X> following led <Y>' <miller>\n",
      "7: 0.999 : <lb> '<X> absorbed following <Y>' <act>\n",
      "8: 0.999 : <branch> '<X> named after <Y>' <them>\n",
      "9: 0.999 : <thugs> '<X> sends after <Y>' <her>\n",
      "10: 0.999 : <opponent> '<X> after failed <Y>' <test>\n",
      "11: 0.999 : <rest> '<X> after returned <Y>' <regiment>\n",
      "12: 0.999 : <burn> '<X> sustains after <Y>' <spilling>\n",
      "13: 0.999 : <ba> '<X> followed from <Y>' <university>\n",
      "14: 0.999 : <he> '<X> composed after <Y>' <trinity>\n",
      "15: 0.999 : <country> '<X> moved into <Y>' <century>\n",
      "16: 0.999 : <student> '<X> fell into <Y>' <river>\n",
      "17: 0.999 : <period> '<X> after transferred <Y>' <he>\n",
      "18: 0.999 : <hell> '<X> fell from <Y>' <sky>\n",
      "19: 0.999 : <month> '<X> after dealt <Y>' <he>\n",
      "20: 0.999 : <who> '<X> dismissed from <Y>' <group>\n",
      "21: 0.999 : <football> '<X> after received <Y>' <degree>\n",
      "22: 0.999 : <temples> '<X> fallen into <Y>' <disrepair>\n",
      "23: 0.999 : <workers> '<X> pushed into <Y>' <depression>\n",
      "24: 0.999 : <illinois> '<X> at led <Y>' <he>\n",
      "25: 0.999 : <allies> '<X> broke in <Y>' <may>\n",
      "26: 0.999 : <injury> '<X> resulted in <Y>' <operations>\n",
      "27: 0.999 : <massacre> '<X> resulted in <Y>' <onslaught>\n",
      "28: 0.999 : <demonstrators> '<X> descended from <Y>' <vehicles>\n",
      "29: 0.999 : <competition> '<X> resulted in <Y>' <lands>\n",
      "30: 0.999 : <boy> '<X> went into <Y>' <bathroom>\n",
      "31: 0.999 : <puzzle> '<X> emerged from <Y>' <study>\n",
      "32: 0.999 : <knowledge> '<X> gained from <Y>' <recruits>\n",
      "33: 0.999 : <access> '<X> gained from <Y>' <doorway>\n",
      "34: 0.999 : <he> '<X> resigned due <Y>' <opposition>\n",
      "35: 0.999 : <who> '<X> <OOV> after <Y>' <death>\n",
      "36: 0.999 : <settlement> '<X> after appointed <Y>' <he>\n",
      "37: 0.999 : <case> '<X> resulted from <Y>' <raid>\n",
      "38: 0.999 : <line> '<X> ran from <Y>' <istanbul>\n",
      "39: 0.999 : <part> '<X> took in <Y>' <program>\n",
      "40: 0.999 : <periods> '<X> during dropped <Y>' <level>\n",
      "41: 0.999 : <massacre> '<X> after had <Y>' <honor>\n",
      "42: 0.999 : <money> '<X> lost due <Y>' <drop>\n",
      "43: 0.999 : <water> '<X> evaporated from <Y>' <kettle>\n",
      "44: 0.999 : <he> '<X> took in <Y>' <program>\n",
      "45: 0.999 : <turkey> '<X> took in <Y>' <group>\n",
      "46: 0.999 : <regiment> '<X> took in <Y>' <engagement>\n",
      "47: 0.999 : <victim> '<X> dragged into <Y>' <water>\n",
      "48: 0.999 : <resolution> '<X> welcomed in <Y>' <december>\n",
      "49: 0.999 : <figures> '<X> took in <Y>' <innings>\n",
      "================================================================================\n",
      "Validation loss: 2.3452\n",
      "(0:39:51) step 1010/1532, epoch 0 Training Loss = 0.31651 :: 251.792 phrases/sec :: (0:17:16) hours left\n",
      "(0:40:19) step 1020/1532, epoch 0 Training Loss = 0.31712 :: 251.163 phrases/sec :: (0:16:57) hours left\n",
      "(0:40:44) step 1030/1532, epoch 0 Training Loss = 0.31890 :: 250.833 phrases/sec :: (0:16:37) hours left\n",
      "(0:41:7) step 1040/1532, epoch 0 Training Loss = 0.31898 :: 250.646 phrases/sec :: (0:16:16) hours left\n",
      "(0:41:32) step 1050/1532, epoch 0 Training Loss = 0.31865 :: 250.339 phrases/sec :: (0:15:55) hours left\n",
      "(0:41:58) step 1060/1532, epoch 0 Training Loss = 0.31737 :: 249.989 phrases/sec :: (0:15:34) hours left\n",
      "(0:42:22) step 1070/1532, epoch 0 Training Loss = 0.31824 :: 249.747 phrases/sec :: (0:15:13) hours left\n",
      "(0:42:47) step 1080/1532, epoch 0 Training Loss = 0.31588 :: 249.461 phrases/sec :: (0:14:52) hours left\n",
      "(0:43:11) step 1090/1532, epoch 0 Training Loss = 0.31954 :: 249.215 phrases/sec :: (0:14:31) hours left\n",
      "(0:43:36) step 1100/1532, epoch 0 Training Loss = 0.31656 :: 248.964 phrases/sec :: (0:14:10) hours left\n",
      "(0:44:0) step 1110/1532, epoch 0 Training Loss = 0.31644 :: 248.820 phrases/sec :: (0:13:48) hours left\n",
      "(0:44:24) step 1120/1532, epoch 0 Training Loss = 0.32127 :: 248.601 phrases/sec :: (0:13:27) hours left\n",
      "(0:44:47) step 1130/1532, epoch 0 Training Loss = 0.31646 :: 248.470 phrases/sec :: (0:13:5) hours left\n",
      "(0:45:12) step 1140/1532, epoch 0 Training Loss = 0.31770 :: 248.222 phrases/sec :: (0:12:44) hours left\n",
      "(0:45:40) step 1150/1532, epoch 0 Training Loss = 0.31669 :: 247.691 phrases/sec :: (0:12:23) hours left\n",
      "(0:46:4) step 1160/1532, epoch 0 Training Loss = 0.31621 :: 247.486 phrases/sec :: (0:12:2) hours left\n",
      "(0:46:28) step 1170/1532, epoch 0 Training Loss = 0.31563 :: 247.350 phrases/sec :: (0:11:40) hours left\n",
      "(0:46:54) step 1180/1532, epoch 0 Training Loss = 0.31714 :: 246.979 phrases/sec :: (0:11:19) hours left\n",
      "(0:47:19) step 1190/1532, epoch 0 Training Loss = 0.31830 :: 246.792 phrases/sec :: (0:10:57) hours left\n",
      "(0:47:44) step 1200/1532, epoch 0 Training Loss = 0.31740 :: 246.536 phrases/sec :: (0:10:35) hours left\n",
      "================================================================================\n",
      "Top 50 closest phrases to <side> '<X> of <Y>' <road>\n",
      "0: 1.000 : <risk> '<X> of <Y>' <abortion>\n",
      "1: 1.000 : <end> '<X> of <Y>' <game>\n",
      "2: 1.000 : <house> '<X> of <Y>' <county>\n",
      "3: 1.000 : <jungle> '<X> of <Y>' <automotives>\n",
      "4: 1.000 : <hall> '<X> of <Y>' <fame>\n",
      "5: 1.000 : <may> '<X> of <Y>' <year>\n",
      "6: 1.000 : <period> '<X> of <Y>' <years>\n",
      "7: 1.000 : <member> '<X> of <Y>' <faculty>\n",
      "8: 1.000 : <fraternity> '<X> of <Y>' <sections>\n",
      "9: 1.000 : <site> '<X> of <Y>' <history>\n",
      "10: 1.000 : <hamilton> '<X> of <Y>' <court>\n",
      "11: 1.000 : <years> '<X> of <Y>' <tavern>\n",
      "12: 1.000 : <proceedings> '<X> of <Y>' <court>\n",
      "13: 1.000 : <half> '<X> of <Y>' <century>\n",
      "14: 1.000 : <day> '<X> of <Y>' <season>\n",
      "15: 1.000 : <corner> '<X> of <Y>' <house>\n",
      "16: 1.000 : <side> '<X> of <Y>' <road>\n",
      "17: 1.000 : <hiring> '<X> of <Y>' <sanchez>\n",
      "18: 1.000 : <grounds> '<X> of <Y>' <adultery>\n",
      "19: 1.000 : <part> '<X> of <Y>' <plan>\n",
      "20: 1.000 : <forces> '<X> of <Y>' <egypt>\n",
      "21: 1.000 : <middle> '<X> of <Y>' <cell>\n",
      "22: 1.000 : <confederation> '<X> of <Y>' <cantons>\n",
      "23: 1.000 : <beats> '<X> of <Y>' <drums>\n",
      "24: 1.000 : <republic> '<X> of <Y>' <florence>\n",
      "25: 1.000 : <congregation> '<X> of <Y>' <hippos>\n",
      "26: 1.000 : <part> '<X> of <Y>' <hancock>\n",
      "27: 1.000 : <church> '<X> of <Y>' <england>\n",
      "28: 1.000 : <verge> '<X> of <Y>' <victory>\n",
      "29: 1.000 : <post> '<X> of <Y>' <commanding>\n",
      "30: 1.000 : <application> '<X> of <Y>' <criteria>\n",
      "31: 1.000 : <pair> '<X> of <Y>' <wheels>\n",
      "32: 1.000 : <province> '<X> of <Y>' <manitoba>\n",
      "33: 1.000 : <exploitation> '<X> of <Y>' <oil>\n",
      "34: 1.000 : <presence> '<X> of <Y>' <lead>\n",
      "35: 1.000 : <development> '<X> of <Y>' <chart>\n",
      "36: 1.000 : <age> '<X> of <Y>' <%>\n",
      "37: 1.000 : <culture> '<X> of <Y>' <groups>\n",
      "38: 1.000 : <cackle> '<X> of <Y>' <hyenas>\n",
      "39: 1.000 : <zone> '<X> of <Y>' <occupation>\n",
      "40: 1.000 : <roots> '<X> of <Y>' <tree>\n",
      "41: 1.000 : <positions> '<X> of <Y>' <party>\n",
      "42: 1.000 : <context> '<X> of <Y>' <school>\n",
      "43: 1.000 : <end> '<X> of <Y>' <season>\n",
      "44: 1.000 : <lead> '<X> of <Y>' <foxes>\n",
      "45: 1.000 : <chapel> '<X> of <Y>' <bath>\n",
      "46: 1.000 : <expansion> '<X> of <Y>' <web>\n",
      "47: 1.000 : <title> '<X> of <Y>' <valley>\n",
      "48: 1.000 : <arm> '<X> of <Y>' <machine>\n",
      "49: 1.000 : <part> '<X> of <Y>' <package>\n",
      "================================================================================\n",
      "Validation loss: 2.3775\n",
      "(0:48:37) step 1210/1532, epoch 0 Training Loss = 0.31723 :: 246.365 phrases/sec :: (0:9:45) hours left\n",
      "(0:49:1) step 1220/1532, epoch 0 Training Loss = 0.31880 :: 246.175 phrases/sec :: (0:9:23) hours left\n",
      "(0:49:25) step 1230/1532, epoch 0 Training Loss = 0.31623 :: 246.095 phrases/sec :: (0:9:1) hours left\n",
      "(0:49:49) step 1240/1532, epoch 0 Training Loss = 0.31738 :: 245.947 phrases/sec :: (0:8:39) hours left\n",
      "(0:50:13) step 1250/1532, epoch 0 Training Loss = 0.31532 :: 245.828 phrases/sec :: (0:8:16) hours left\n",
      "(0:50:37) step 1260/1532, epoch 0 Training Loss = 0.31792 :: 245.682 phrases/sec :: (0:7:54) hours left\n",
      "(0:51:2) step 1270/1532, epoch 0 Training Loss = 0.31792 :: 245.466 phrases/sec :: (0:7:32) hours left\n",
      "(0:51:27) step 1280/1532, epoch 0 Training Loss = 0.31641 :: 245.213 phrases/sec :: (0:7:10) hours left\n",
      "(0:51:53) step 1290/1532, epoch 0 Training Loss = 0.31663 :: 244.928 phrases/sec :: (0:6:49) hours left\n",
      "(0:52:20) step 1300/1532, epoch 0 Training Loss = 0.31656 :: 244.595 phrases/sec :: (0:6:27) hours left\n",
      "(0:52:44) step 1310/1532, epoch 0 Training Loss = 0.31787 :: 244.452 phrases/sec :: (0:6:4) hours left\n",
      "(0:53:8) step 1320/1532, epoch 0 Training Loss = 0.31675 :: 244.292 phrases/sec :: (0:5:42) hours left\n",
      "(0:53:33) step 1330/1532, epoch 0 Training Loss = 0.31678 :: 244.122 phrases/sec :: (0:5:20) hours left\n",
      "(0:53:58) step 1340/1532, epoch 0 Training Loss = 0.31682 :: 243.979 phrases/sec :: (0:4:58) hours left\n",
      "(0:54:22) step 1350/1532, epoch 0 Training Loss = 0.31645 :: 243.816 phrases/sec :: (0:4:35) hours left\n",
      "(0:54:47) step 1360/1532, epoch 0 Training Loss = 0.31754 :: 243.646 phrases/sec :: (0:4:13) hours left\n",
      "(0:55:13) step 1370/1532, epoch 0 Training Loss = 0.31786 :: 243.377 phrases/sec :: (0:3:51) hours left\n",
      "(0:55:41) step 1380/1532, epoch 0 Training Loss = 0.31559 :: 242.995 phrases/sec :: (0:3:28) hours left\n",
      "(0:56:8) step 1390/1532, epoch 0 Training Loss = 0.31569 :: 242.644 phrases/sec :: (0:3:6) hours left\n",
      "(0:56:37) step 1400/1532, epoch 0 Training Loss = 0.31732 :: 242.172 phrases/sec :: (0:2:44) hours left\n",
      "================================================================================\n",
      "Top 50 closest phrases to <households> '<X> were had householder with <Y>' <husband>\n",
      "0: 1.000 : <households> '<X> were had householder with <Y>' <husband>\n",
      "1: 1.000 : <households> '<X> were had householder with <Y>' <husband>\n",
      "2: 1.000 : <households> '<X> were had householder with <Y>' <husband>\n",
      "3: 1.000 : <households> '<X> were had householder with <Y>' <husband>\n",
      "4: 1.000 : <couples> '<X> were had householder with <Y>' <husband>\n",
      "5: 1.000 : <husband> '<X> with householder had were <Y>' <%>\n",
      "6: 1.000 : <husband> '<X> with householder had were <Y>' <%>\n",
      "7: 1.000 : <husband> '<X> with householder had were <Y>' <%>\n",
      "8: 1.000 : <husband> '<X> with householder had were <Y>' <%>\n",
      "9: 1.000 : <husband> '<X> with householder had were <Y>' <families>\n",
      "10: 0.999 : <%> '<X> had householder with <Y>' <husband>\n",
      "11: 0.999 : <%> '<X> had householder with <Y>' <husband>\n",
      "12: 0.999 : <%> '<X> had householder with <Y>' <husband>\n",
      "13: 0.999 : <%> '<X> had householder with <Y>' <husband>\n",
      "14: 0.999 : <%> '<X> had householder with <Y>' <husband>\n",
      "15: 0.999 : <%> '<X> had householder with <Y>' <husband>\n",
      "16: 0.999 : <dragons> '<X> with run had with <Y>' <distinction>\n",
      "17: 0.999 : <%> '<X> had children living with <Y>' <them>\n",
      "18: 0.999 : <%> '<X> had children living with <Y>' <them>\n",
      "19: 0.999 : <%> '<X> had children living with <Y>' <them>\n",
      "20: 0.999 : <households> '<X> were had children living with <Y>' <them>\n",
      "21: 0.999 : <result> '<X> to came were women with <Y>' <interval>\n",
      "22: 0.999 : <children> '<X> had households were had <Y>' <householder>\n",
      "23: 0.999 : <interval> '<X> with women compared with <Y>' <months>\n",
      "24: 0.999 : <children> '<X> had households were had <Y>' <%>\n",
      "25: 0.999 : <%> '<X> had households were had <Y>' <%>\n",
      "26: 0.999 : <%> '<X> had households were had <Y>' <%>\n",
      "27: 0.999 : <pregnancy> '<X> to women were compared with <Y>' <%>\n",
      "28: 0.999 : <school> '<X> for parents worked with <Y>' <government>\n",
      "29: 0.999 : <%> '<X> had households were had <Y>' <%>\n",
      "30: 0.999 : <households> '<X> were had children under <Y>' <age>\n",
      "31: 0.999 : <households> '<X> were had children under <Y>' <age>\n",
      "32: 0.999 : <households> '<X> were had children under <Y>' <age>\n",
      "33: 0.999 : <panels> '<X> with grey were in <Y>' <teens>\n",
      "34: 0.999 : <mountains> '<X> had are home among <Y>' <others>\n",
      "35: 0.999 : <presley> '<X> had participants began careers with <Y>' <records>\n",
      "36: 0.999 : <agreement> '<X> in is have people with <Y>' <antibodies>\n",
      "37: 0.999 : <%> '<X> with was with in <Y>' <region>\n",
      "38: 0.999 : <college> '<X> celebrated graduated woman with <Y>' <bachelor>\n",
      "39: 0.999 : <americans> '<X> by attacked escaped with <Y>' <life>\n",
      "40: 0.999 : <staff> '<X> dismissed for reason connected with <Y>' <arrangement>\n",
      "41: 0.999 : <amenities> '<X> as sales station with <Y>' <shop>\n",
      "42: 0.999 : <commission> '<X> on seat having with <Y>' <exception>\n",
      "43: 0.999 : <minutes> '<X> after retreated killed with <Y>' <shot>\n",
      "44: 0.999 : <bass> '<X> was elected finishing with <Y>' <votes>\n",
      "45: 0.999 : <mammal> '<X> breaks by hitting with <Y>' <stone>\n",
      "46: 0.999 : <it> '<X> game made answered with <Y>' <goal>\n",
      "47: 0.999 : <purpose> '<X> known made by <OOV> with <Y>' <culture>\n",
      "48: 0.999 : <smith> '<X> <OOV> were with <Y>' <club>\n",
      "49: 0.999 : <southeast> '<X> east to areas are with <Y>' <grassland>\n",
      "================================================================================\n",
      "Validation loss: 2.3758\n",
      "(0:57:35) step 1410/1532, epoch 0 Training Loss = 0.31572 :: 242.026 phrases/sec :: (0:1:48) hours left\n",
      "(0:57:59) step 1420/1532, epoch 0 Training Loss = 0.31691 :: 241.913 phrases/sec :: (0:1:25) hours left\n",
      "(0:58:23) step 1430/1532, epoch 0 Training Loss = 0.31592 :: 241.851 phrases/sec :: (0:1:2) hours left\n",
      "(0:58:48) step 1440/1532, epoch 0 Training Loss = 0.31792 :: 241.725 phrases/sec :: (0:0:40) hours left\n",
      "(0:59:12) step 1450/1532, epoch 0 Training Loss = 0.31505 :: 241.610 phrases/sec :: (0:0:17) hours left\n",
      "(0:59:36) step 1460/1532, epoch 0 Training Loss = 0.31544 :: 241.506 phrases/sec :: (-1:59:54) hours left\n",
      "(1:0:0) step 1470/1532, epoch 0 Training Loss = 0.31662 :: 241.459 phrases/sec :: (-1:59:31) hours left\n",
      "Saving model...\n",
      "Saving model to file: checkpoints/renormalize.ckpt-1473-0\n",
      "(1:0:29) step 1480/1532, epoch 0 Training Loss = 0.31645 :: 241.337 phrases/sec :: (-1:59:4) hours left\n",
      "(1:0:53) step 1490/1532, epoch 0 Training Loss = 0.31427 :: 241.270 phrases/sec :: (-1:58:41) hours left\n",
      "(1:1:18) step 1500/1532, epoch 0 Training Loss = 0.31694 :: 241.158 phrases/sec :: (-1:58:18) hours left\n",
      "(1:1:42) step 1510/1532, epoch 0 Training Loss = 0.31603 :: 241.081 phrases/sec :: (-1:57:55) hours left\n",
      "(1:2:5) step 1520/1532, epoch 0 Training Loss = 0.31750 :: 241.028 phrases/sec :: (-1:57:32) hours left\n",
      "(1:2:30) step 1530/1532, epoch 0 Training Loss = 0.31634 :: 240.921 phrases/sec :: (-1:57:9) hours left\n",
      "Saving model to file: checkpoints/renormalize.ckpt-1532-0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'checkpoints/renormalize.ckpt-1532-0'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(nn)\n",
    "drnn = reset_drnn(model_name='renormalize', bi=False, max_to_keep=0)\n",
    "\n",
    "# hyperparameters\n",
    "num_epochs = 1\n",
    "batch_size =50\n",
    "neg_per = 10\n",
    "neg_level= 1\n",
    "target_neg = True\n",
    "\n",
    "num_nearby = 50\n",
    "nearby_mod = 200\n",
    "sample_power = .75\n",
    "DH.scale_vocab_dist(sample_power)\n",
    "DH.scale_target_dist(.5)\n",
    "\n",
    "# bookkeeping\n",
    "num_steps = DH.num_steps(batch_size)\n",
    "total_step = 1\n",
    "save_interval = 30 * 60 # half hour in seconds\n",
    "save_time = time()\n",
    "\n",
    "#timing stuff\n",
    "start = time()\n",
    "fit_time = 0\n",
    "nearby_time = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    offset = 0 #if epoch else 400\n",
    "    DH.shuffle_data()\n",
    "    for step , batch in enumerate(DH.batches(batch_size, offset=offset, \n",
    "                                             neg_per=neg_per, neg_level=neg_level, target_neg=target_neg)):\n",
    "        inputs, targets, labels, lengths, _ = batch\n",
    "        if not step: step = offset\n",
    "        t0 = time()\n",
    "        loss = drnn.partial_unsup_fit(inputs, targets, labels, lengths)\n",
    "        fit_time = (fit_time * float(total_step) +  time() - t0) / (total_step + 1) # running average\n",
    "        if step % 10 == 0:\n",
    "            m,s = divmod(time()-start, 60)\n",
    "            h,m = divmod(m, 60)\n",
    "            left = time_left(num_epochs, num_steps, fit_time, nearby_time, start, nearby_mod)\n",
    "            ml,sl = divmod(left, 60)\n",
    "            hl,ml = divmod(ml, 60)\n",
    "            pps = batch_size*(neg_per + 1) / fit_time \n",
    "            print(\"(%i:%i:%i) step %i/%i, epoch %i Training Loss = %1.5f :: %0.3f phrases/sec :: (%i:%i:%i) hours left\" \n",
    "                  % (h,m,s, step, num_steps, epoch, loss, pps, hl, ml, sl))\n",
    "        if (total_step-1) % nearby_mod == 0: # do one right away so we get a good timing estimate\n",
    "            t0 = time()\n",
    "            run_validation_test(num_nearby) # check out the nearby phrases in the validation set\n",
    "            inputs, targets, labels, lengths, _ = DH.validation_batch()\n",
    "            valid_loss = drnn.validation_loss(inputs, targets, labels, lengths)\n",
    "            print(\"Validation loss: %0.4f\" % valid_loss)\n",
    "            nearby_time = (nearby_time * float(total_step) + time() - t0) / (total_step + 1) # running average\n",
    "\n",
    "        if (time() - save_time) > save_interval:\n",
    "            print(\"Saving model...\")\n",
    "            drnn.checkpoint()\n",
    "            save_time = time()\n",
    "        total_step +=1\n",
    "drnn.checkpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-100, 10000)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZQAAAEACAYAAACUMoD1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJgwUSVITBookqQkDRZLUxP8HjCbJwlmvYggAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x15c8e4290>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DH.scale_target_dist(.5)\n",
    "plt.bar(range(len(DH._target_dist)), sorted(DH._target_dist, reverse=True))\n",
    "plt.xlim(-100,10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
