{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from spacy.en import English\n",
    "nlp = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_raw_x(line, verbose=False):\n",
    "    \"\"\"Convert raw line of semeval data into a useable form\n",
    "    \n",
    "    Convert to a triple of (spacy sentence, e1_token, e2_token)\n",
    "    \"\"\"\n",
    "    if isinstance(line, str):\n",
    "        line = unicode(line)\n",
    "    s = line.strip()\n",
    "    s = s[s.index(u'\"')+1: -(s[::-1].index(u'\"')+1)] # get s between first \" and last \"\n",
    "    # we will assume that the first token follow the <e1> , <e2> tags are the entity words.  \n",
    "    # note this is a big assumption and hopefully phrases will be in subtrees or in heads of the parse trees\n",
    "    # TODO: this can be addressed by making it a 5-tuple with the endpoints also encoded\n",
    "    \n",
    "    # sometimes the tags are missing spaces in front or behind.\n",
    "    # check out those cases separately so we don't add exrta whitespace and mess up parsing\n",
    "    # Proper whitespaceing case\n",
    "    s = s.replace(u' <e1>', u' e1>') # make sure there's spacing so it's recognized as seperate token\n",
    "    s = s.replace(u'</e1> ', u' ')    # drop right tag\n",
    "    s = s.replace(u' <e2>', u' e2>')\n",
    "    s = s.replace(u'</e2> ', u' ')\n",
    "    # if there wasn't proper whitespacing, the previous code didn't run\n",
    "    # so fill in the gaps with these corner cases where we add in extra whitespace\n",
    "    s = s.replace(u'<e1>', u' e1>') # make sure there's spacing so it's recognized as seperate token\n",
    "    s = s.replace(u'</e1>', u' ')    # drop right tag\n",
    "    s = s.replace(u'<e2>', u' e2>')\n",
    "    s = s.replace(u'</e2>', u' ')\n",
    "    \n",
    "    s = nlp(s)\n",
    "    tokenized_s = [token.text for token in s]\n",
    "    for i, token in enumerate(tokenized_s):\n",
    "        if u'e1>' == token[:3]:\n",
    "            tokenized_s[i] = token[3:]\n",
    "            e1_index = i\n",
    "        elif u'e2>' == token[:3]:\n",
    "            tokenized_s[i] = token[3:]\n",
    "            e2_index = i\n",
    "    s = u' '.join(tokenized_s)\n",
    "    s = nlp(s)\n",
    "    e1 = s[e1_index]\n",
    "    e2 = s[e2_index]\n",
    "    return (s, e1, e2)\n",
    "\n",
    "def dependency_path_to_root(token):\n",
    "    \"\"\"Traverse up the dependency tree. Include the token we are tracing\"\"\"\n",
    "    dep_path = [token]\n",
    "    while token.head is not token:\n",
    "        dep_path.append(token.head)\n",
    "        token = token.head\n",
    "    # dep_path.append(token.head) # add the root node\n",
    "    return dep_path\n",
    "\n",
    "def find_common_ancestor(e1_path, e2_path, verbose=False):\n",
    "    \"\"\"Loop through both dep paths and return common ancestor\"\"\"\n",
    "    for t1 in e1_path:\n",
    "        for t2 in e2_path:\n",
    "            if verbose:\n",
    "                print(t1, t2)\n",
    "            if t1.idx ==  t2.idx:\n",
    "                if verbose:\n",
    "                    print(\"Common found!\")\n",
    "                return t1\n",
    "    return None\n",
    "\n",
    "def convert_nominals_to_sdp(X, Y, verbose=False):\n",
    "    X_path = dependency_path_to_root(X)\n",
    "    Y_path = dependency_path_to_root(Y)\n",
    "    if verbose:\n",
    "        print(X.text, X.dep_)\n",
    "        print(X_path)\n",
    "        print(Y.text, Y.dep_)\n",
    "        print(Y_path)\n",
    "    common = find_common_ancestor(X_path, Y_path, verbose=verbose)\n",
    "#     # now we don't want nouns for assembly\n",
    "#     X_path = X_path[1:]\n",
    "#     Y_path = Y_path[1:]\n",
    "    # CASE (1)\n",
    "    if not common:\n",
    "        print(\"Didn't find common ancestor\")\n",
    "        return None\n",
    "    # CASE (2)\n",
    "    elif X is common:\n",
    "        sdp = []\n",
    "        for token in Y_path:        # looks like (Y <- ... <- X <-) ...\n",
    "            sdp.append((token.text.lower(), token.dep_))\n",
    "            if token is common:     # stop after X\n",
    "                break\n",
    "        sdp = list(reversed(sdp))   # flip to get ... (-> X -> ... -> Y)\n",
    "    elif Y is common:\n",
    "        sdp = []\n",
    "        for token in X_path:        # looks like (X <- ... <- Y <- ) ...\n",
    "            sdp.append((token.text.lower(), token.dep_))\n",
    "            if token is common:     # stop after Y\n",
    "                  break\n",
    "    # CASE (3)\n",
    "    else:\n",
    "        sdp = []\n",
    "        for token in (X_path):      # looks like (X <- ... <- Z <-) ...\n",
    "            sdp.append((token.text.lower(), token.dep_))\n",
    "            if token is common:     # keep Z this time\n",
    "                break\n",
    "        ysdp = []                   # need to keep track of seperate, then will reverse and extend later\n",
    "        for token in Y_path:        # looks like (Y <- ... <-) Z <- ... \n",
    "            if token is common:     # don't keep Z from this side\n",
    "                break\n",
    "            ysdp.append((token.text.lower(), token.dep_))\n",
    "        sdp.extend(list(reversed(ysdp))) # looks like (X <- ... <- Z -> ... ) -> Y)\n",
    "    # convert endpoints of the paths to placeholder X and Y tokens\n",
    "    sdp[0] = (u'<X>', sdp[0][1])\n",
    "    sdp[-1] = (u'<Y>', sdp[-1][1])\n",
    "#     if len(sdp) < min_len or len(sdp) > max_len:\n",
    "#         continue                    # skip ones that are too short or long\n",
    "    return {'path': sdp, 'target':(X.text.lower(), Y.text.lower())}\n",
    "\n",
    "def post_process_sdp(sdp):\n",
    "    \"\"\" Filter out unwanted sdps structure \"\"\"\n",
    "    if not sdp:\n",
    "        return sdp\n",
    "    bad_tokens = set([',', '.', '-', '(', ')', '&', '*', '_', '%', '!', '?', '/', '<', '>', '\\\\', '[', ']', '{', '}', '\"', \"'\"])\n",
    "    sdp['path'] = [x for x in sdp['path'] if x[0] not in bad_tokens]\n",
    "    return sdp\n",
    "\n",
    "def is_ok_sdp(sdp):#, int2vocab, oov_percent=75):\n",
    "    \"\"\" Helper function to mak sure SDP isn't a poor example.\n",
    "\n",
    "    Filters used to identify bas data:\n",
    "    1. Neither targets may be oov\n",
    "    2. The relation itself must be less than `oov_percent` percent number of relations\n",
    "    \"\"\"\n",
    "#     oov = int2vocab.keys()[-1]\n",
    "#     # print(oov, sdp['target'])\n",
    "#     if sdp['target'][0] == oov or sdp['target'][1] == oov:\n",
    "#         return False\n",
    "#     oov_count = len([ t for t in sdp['path'] if t[0] == oov])\n",
    "#     too_many = int((oov_percent/100.0)*len(sdp['path']))\n",
    "#     if oov_count > too_many:\n",
    "#         return False\n",
    "    if not sdp or not sdp['path'] or not sdp['target']:\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def line_to_data(raw_line, verbose=False):\n",
    "    sent = convert_raw_x(raw_line)\n",
    "    e1 = sent[1]\n",
    "    e2 = sent[2]\n",
    "    sdp = convert_nominals_to_sdp(e1, e2, verbose=verbose)\n",
    "    if not sdp:\n",
    "        print(raw_line)\n",
    "        print(sent)\n",
    "#     post_process_sdp(sdp)\n",
    "    if is_ok_sdp(sdp):\n",
    "        return sent, sdp['path'], sdp['target']\n",
    "    else:\n",
    "        print(\"Bad sentence: %r\" % raw_line )\n",
    "        print(sent, sdp)\n",
    "        return None, None, None\n",
    "\n",
    "def line_to_label(raw_label_line, label2int):\n",
    "    \"\"\"Convert raw line of semeval labels into a useable form (ints)\"\"\"\n",
    "    line = raw_label_line.strip()\n",
    "    if line in label2int:\n",
    "        return label2int[line]\n",
    "    else:\n",
    "        label2int[line] = len(label2int.keys())\n",
    "        return label2int[line]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_semeval_data():\n",
    "    \"\"\"Load in SemEval 2010 Task 8 Training file and return lists of tuples:\n",
    "    \n",
    "    Tuple form =  (spacy(stripped sentence), index of e1, index of e2)\"\"\"\n",
    "    ### TRAINING AND VALIDATION DATA ###\n",
    "    training_txt_file = 'SemEval2010_task8_all_data/SemEval2010_task8_training/TRAIN_FILE.TXT'\n",
    "    validation_index = 8000 - 891# len data - len valid - 1 since we start at 0\n",
    "    train = {'raws':[], 'sents':[], 'sdps':[], 'targets':[], 'labels':[]}\n",
    "    valid = {'raws':[], 'sents':[], 'sdps':[], 'targets':[], 'labels':[]}\n",
    "    text = open(training_txt_file, 'r').readlines()\n",
    "    label2int = dict() # keep running dictionary of labels\n",
    "    assert len(text) // 4 == 8000\n",
    "    for cursor in range(len(text) // 4): # each 4 lines is a datum\n",
    "            text_line = text[4*cursor]\n",
    "            label_line = text[4*cursor +1]\n",
    "            sent, sdp, target = line_to_data(text_line)\n",
    "            label = line_to_label(label_line, label2int)\n",
    "#             print(sent, sdp, target, label)\n",
    "            if not (sent and sdp and target):\n",
    "                print(\"Skipping this one... %r\" % text_line)\n",
    "                print(sent, sdp, target, label)\n",
    "                continue\n",
    "            if cursor < validation_index:\n",
    "                train['raws'].append(text_line)\n",
    "                train['sents'].append(sent)\n",
    "                train['sdps'].append(sdp)\n",
    "                train['targets'].append(target)\n",
    "                train['labels'].append(label)\n",
    "            else:\n",
    "                valid['raws'].append(text_line)\n",
    "                valid['sents'].append(sent)\n",
    "                valid['sdps'].append(sdp)\n",
    "                valid['targets'].append(target)\n",
    "                valid['labels'].append(label)\n",
    "\n",
    "    print(\"Num training: %i\" % len(train['labels']))\n",
    "    print(\"Num valididation: %i\" % len(valid['labels']))\n",
    "    assert sorted(label2int.values()) == range(19) # 2 for each 9 asymmetric relations and 1 other\n",
    "    \n",
    "    ### TEST DATA ### (has no labels)\n",
    "    test_txt_file = \"SemEval2010_task8_all_data/SemEval2010_task8_testing/TEST_FILE.txt\"\n",
    "    test = {'raws':[], 'sents':[], 'sdps':[], 'targets':[]}\n",
    "    text = open(test_txt_file, 'r').readlines()\n",
    "    for line in text:\n",
    "        send, sdp, target = line_to_data(line)\n",
    "        test['raws'].append(line)\n",
    "        test['sents'].append(sent)\n",
    "        test['sdps'].append(sdp)\n",
    "        test['targets'].append(target)\n",
    "    \n",
    "    print(\"Num testing: %i\" % len(test['targets']))\n",
    "    return train, valid, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Didn't find common ancestor\n",
      "1790\t\"The imams were removed from a US Airways <e1>flight</e1> awaiting departure from the Minneapolis-St. Paul <e2>airport</e2>.\"\n",
      "\n",
      "(The imams were removed from a US Airways flight awaiting departure from the Minneapolis - St . Paul airport ., flight , airport )\n",
      "Bad sentence: '1790\\t\"The imams were removed from a US Airways <e1>flight</e1> awaiting departure from the Minneapolis-St. Paul <e2>airport</e2>.\"\\r\\n'\n",
      "((The imams were removed from a US Airways flight awaiting departure from the Minneapolis - St . Paul airport ., flight , airport ), None)\n",
      "Skipping this one... '1790\\t\"The imams were removed from a US Airways <e1>flight</e1> awaiting departure from the Minneapolis-St. Paul <e2>airport</e2>.\"\\r\\n'\n",
      "(None, None, None, 10)\n",
      "\n",
      "\n",
      "Num training: 7108\n",
      "Num valididation: 891\n",
      "Didn't find common ancestor\n",
      "8310\t\"Tributes have been paid to the <e1>writer</e1> who created Goodness Gracious Me, the hit BBC television <e2>series</e2>.\"\n",
      "\n",
      "(Tributes have been paid to the writer who created Goodness Gracious Me , the hit BBC television series ., writer , series )\n",
      "Bad sentence: '8310\\t\"Tributes have been paid to the <e1>writer</e1> who created Goodness Gracious Me, the hit BBC television <e2>series</e2>.\"\\r\\n'\n",
      "((Tributes have been paid to the writer who created Goodness Gracious Me , the hit BBC television series ., writer , series ), None)\n",
      "Num testing: 2717\n"
     ]
    }
   ],
   "source": [
    "train, valid, test = load_semeval_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "line = u'1708    \"The English writer, poet, philologist, and university <e1>professor</e1> was best known as the author of the classic high fantasy <e2>works</e2> The Hobbit, The Lord of the Rings and The Silmarillion.\"'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The English writer, poet, philologist, and university e1>professor was best known as the author of the classic high fantasy e2>works The Hobbit, The Lord of the Rings and The Silmarillion.\n",
      "The English writer , poet , philologist , and university professor was best known as the author of the classic high fantasy works The Hobbit , The Lord of the Rings and The Silmarillion .\n",
      "(u'professor', u'appos')\n",
      "[professor , writer , was , known ]\n",
      "(u'works', u'pobj')\n",
      "[works , of , author , as , known ]\n",
      "(professor , works )\n",
      "(professor , of )\n",
      "(professor , author )\n",
      "(professor , as )\n",
      "(professor , known )\n",
      "(writer , works )\n",
      "(writer , of )\n",
      "(writer , author )\n",
      "(writer , as )\n",
      "(writer , known )\n",
      "(was , works )\n",
      "(was , of )\n",
      "(was , author )\n",
      "(was , as )\n",
      "(was , known )\n",
      "(known , works )\n",
      "(known , of )\n",
      "(known , author )\n",
      "(known , as )\n",
      "(known , known )\n",
      "Common found!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((The English writer , poet , philologist , and university professor was best known as the author of the classic high fantasy works The Hobbit , The Lord of the Rings and The Silmarillion .,\n",
       "  professor ,\n",
       "  works ),\n",
       " [(u'<X>', u'appos'),\n",
       "  (u'writer', u'nsubj'),\n",
       "  (u'was', u'auxpass'),\n",
       "  (u'known', u'ROOT'),\n",
       "  (u'as', u'prep'),\n",
       "  (u'author', u'pobj'),\n",
       "  (u'of', u'prep'),\n",
       "  (u'<Y>', u'pobj')],\n",
       " (u'professor', u'works'))"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line_to_data(line, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "line = u'23    \"The <e1>singer</e1>, who performed three of the nominated songs, also caused a <e2>commotion</e2> on the red carpet.\"'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[The , singer , , , who , performed , three , of , the , nominated , songs , , , also , caused , a , commotion , on , the , red , carpet , .]\n",
      "u'singer'\n",
      "u'commotion'\n"
     ]
    }
   ],
   "source": [
    "def convert_raw_x(line):\n",
    "    \"\"\"Convert raw line of semeval data into a useable form\n",
    "    \n",
    "    Convert to a triple of (spacy sentence, e1_token, e2_token)\n",
    "    \"\"\"\n",
    "    s = line.strip()\n",
    "    #print(s)\n",
    "    s = s[s.index('\"')+1: -(s[::-1].index('\"')+1)] # get s between first \" and last \"\n",
    "    # we will assume that the first token follow the <e1> , <e2> tags are the entity words.  \n",
    "    # note this is a big assumption and hopefully phrases will be in subtrees or in heads of the parse trees\n",
    "    # TODO: this can be addressed by making it a 5-tuple with the endpoints also encoded\n",
    "#     s = [ w for w in s.split(' ') if w is not '' ]\n",
    "    s = s.replace('<e1>', 'e1>')\n",
    "    s = s.replace('</e1>', '')\n",
    "    s = s.replace('<e2>', 'e2>')\n",
    "    s = s.replace('</e2>', '')\n",
    "    s = nlp(s)\n",
    "    tokenized_s = [token.text for token in s]\n",
    "    for i, token in enumerate(tokenized_s):\n",
    "        if 'e1>' == token[:3]:\n",
    "            tokenized_s[i] = token[3:]\n",
    "            e1_index = i\n",
    "        elif 'e2>' == token[:3]:\n",
    "            tokenized_s[i] = token[3:]\n",
    "            e2_index = i\n",
    "    s = nlp(u' '.join(tokenized_s))\n",
    "    e1 = s[e1_index]\n",
    "    e2 = s[e2_index]\n",
    "\n",
    "    return (s, e1, e2)\n",
    "\n",
    "s, e1, e2  = convert_raw_x(line)\n",
    "print(\"%r\" % list(s))\n",
    "print(\"%r\" %e1.text)\n",
    "print(\"%r\" % e2.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
